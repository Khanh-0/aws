[{"uri":"https://khanh-0.github.io/aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Dinh Viet Vinh Khanh\nPhone Number: 0353513278\nEmail: khanhdvvse181518@fpt.edu.vn\nUniversity: Ho Chi Minh City FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: GenAI Intern\nInternship Duration: From 8/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.1-event1/","title":"Kick-off AWS First Cloud Journey Workforce","tags":[],"description":"","content":"Event Objectives Celebrate the students who successfully joined the AWS First Cloud Journey – Training on the Job (OJT FALL 2025) program Mark the beginning of a structured learning journey and real-world cloud experience with Amazon Web Services (AWS) Equip participants with hands-on skills in Cloud, DevOps, Security, AI/ML, and Data \u0026amp; Analytics Connect students with the AWS Study Group community (47,000+ members) and AWS partner companies Build a strong bridge between knowledge – technology – career, empowering a new generation of AWS Builders in Vietnam Speakers Nguyen Tran Phuoc Bao – Head of Corporate Relations, FPT University Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Do Huy Thang – DevOps Lead, VNG Danh Hoang Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights Launching the AWS First Cloud Journey Workforce Program The first time I have seen such amazing people doing amazing activities in AWS HCMC. It\u0026rsquo;s such an amazing thing that I have ever witnessed.\nThe event marked the official kickoff for more than 150 students in the OJT Fall 2025 cohort.\nSince its inception in 2021, the AWS First Cloud Journey has supported 2,000+ students, with many alumni now working at leading technology companies in Vietnam and abroad.\nThroughout the kickoff, speakers shared insights on:\nThe future of Cloud Computing in Vietnam Career pathways in DevOps, Cloud, and AI/ML Workforce trends and market demands in the coming years Event Agenda Overview 8:30 – 9:00 | Check-in, Networking \u0026amp; Group Photos 9:00 – 9:15 | Opening remarks from FPT University 9:15 – 9:40 | AWS First Cloud Journey \u0026amp; Future Direction – Nguyen Gia Hung 9:40 – 10:05 | DevOps \u0026amp; Career Opportunities – Do Huy Thang 10:05 – 10:20 | Tea Break \u0026amp; Networking 10:20 – 10:40 | From FCJ to GenAI Engineer – Danh Hoang Hieu Nghi 10:40 – 11:00 | She in Tech – The FCJ Journey – Bui Ho Linh Nhi 11:00 – 11:20 | A Day in the Life of a Cloud Engineer – Pham Nguyen Hai Anh 11:20 – 11:40 | Becoming a Cloud Engineer – Nguyen Dong Thanh Hiep 11:40 – 12:00 | Q\u0026amp;A Session \u0026amp; Final Group Photos Key Takeaways Career Insights \u0026amp; Development Strategy Cloud and DevOps remain among the most in-demand technology fields Speakers emphasized essential skills: Cloud fundamentals (IAM, VPC, Compute, Storage…) CI/CD \u0026amp; DevOps mindset Analytical thinking and problem-solving A recommended learning path for students: Cloud Foundation → Hands-on Projects → DevOps Tools → Specialization (AI/ML, Security, Data) Industry \u0026amp; Alumni Perspectives AWS reaffirmed its mission to build the next generation of AWS Builders in Vietnam Tech companies highlighted hiring needs in: Cloud Engineering DevOps Engineering AI/ML Engineering Data Engineering Alumni shared how self-learning, hands-on practice, and community engagement shaped their success. Applying to Work Start building AWS fundamentals: IAM, EC2, S3, VPC Begin practicing with DevOps tools: Git, Docker, CI/CD (GitHub Actions) Develop mini-projects and write technical documentation Join AWS Study Group events, workshops, and mentoring sessions Gradually explore advanced topics: Serverless, Containers, IaC, AI/ML Event Experience As an intern of the AWS FCJ HCMC Team, attending the Kick-off AWS First Cloud Journey Workforce OJT FALL 2025 was an inspiring and motivating experience.\nWhat I learned from the event A clearer understanding of the Cloud \u0026amp; DevOps career landscape in Vietnam Valuable inspiration from FCJ alumni who have successfully built their careers Essential mindsets emphasized throughout the event: Self-learning Experimentation Embracing challenges The importance of networking and staying connected with peers and mentors Networking \u0026amp; Community Met mentors, speakers, AWS specialists, and engineers working in cloud, DevOps, and AI Built a strong network with teammates and fellow FCJ participants The kickoff event not only provided career insights and technical direction, but also fueled my motivation to begin my journey toward becoming an AWS Builder.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 09/09/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic budget types: + Cost budget + Usage budget + Saving plans budget + Reservation budget 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn and practice: + AWS support packages + Change AWS support packages + Manage support request 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute: Used to run applications, process data, create virtual server environments or containers. Main services: EC2 (Elastic Compute Cloud): Virtual servers running on AWS (like VPS but more flexible). Lambda: Run serverless code, no server management needed, pay per execution. ECS/EKS (Elastic Container Service / Elastic Kubernetes Service): Manage containers (Docker/K8s). Elastic Beanstalk: Automatic app deployment (like PaaS)\nStorage: Store data, from small files to Big Data. S3 (Simple Storage Service): Object storage – used for files, images, videos. EBS (Elastic Block Store): Block storage (used with EC2 like hard drives). EFS (Elastic File System): File system storage, multiple machines can access simultaneously. Glacier: Long-term storage, cheap (for backup/archive).\nNetworking: Connect services, secure and distribute applications. VPC (Virtual Private Cloud): Create private virtual network in AWS (like private data center). Route 53: DNS service, domain management. CloudFront: CDN distributes content faster to global users. ELB (Elastic Load Balancer): Load balancing between multiple servers. API Gateway: Manage and secure APIs.\nDatabase: Store and manage structured/unstructured data. RDS (Relational Database Service): Manage relational databases (MySQL, PostgreSQL, Oracle, SQL Server). Aurora: AWS-developed database, MySQL/PostgreSQL compatible, faster than RDS. DynamoDB: NoSQL Database (non-relational), high speed, auto-scaling. Redshift: Data warehouse for big data analysis. ElastiCache: Data caching (Redis/Memcached).\nSuccessfully created and configured AWS Free Tier account.\nBecame familiar with AWS Management Console and learned how to find, access, and use services via web interface.\nInstalled and configured AWS CLI on computer including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve list of regions View EC2 service Create and manage key pairs Check information about running services "},{"uri":"https://khanh-0.github.io/aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will find my worklog documenting my AWS learning journey. I completed this program over 12 weeks (approximately 3 months), systematically learning and practicing AWS services from fundamentals to advanced topics.\nThroughout this internship period, I progressed from basic AWS concepts to designing and implementing complete cloud architectures. Each week focused on specific AWS services and hands-on practice, building upon previous knowledge to develop comprehensive cloud computing skills.\nBelow is my weekly learning progression:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learning AWS Virtual Private Cloud (VPC)\nWeek 3: Learning Amazon EC2 and compute services\nWeek 4: Understanding AWS IAM and EC2 Instance Storage\nWeek 5: Learning High Availability, Scalability, and Database services\nWeek 6: Learning Amazon Route 53 and Classic Solutions Architecture\nWeek 7: Understanding Amazon S3 and storage features\nWeek 8: Learning CloudFront, Global Accelerator, and AWS Integration \u0026amp; Messaging\nWeek 9: Learning Containers and Serverless architectures\nWeek 10: Understanding Databases, Data \u0026amp; Analytics, and Machine Learning services\nWeek 11: Learning AWS Monitoring, Security, and Advanced Identity\nWeek 12: Learning Disaster Recovery, Migration strategies, and comprehensive review\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.3-architecture/5.3.1-agentcore-memory/","title":"AgentCore Memory","tags":[],"description":"","content":"Configuring Memory in AgentCore To enable memory for AgentCore, follow the steps below.\n1. Create Memory in Bedrock Go to Bedrock → select AgentCore. Switch to the Memory tab. Click Create memory. In the memory creation interface, you will see the following sections:\nMemory name Set the name for the memory that AgentCore will use.\nShort-term memory (raw event) expiration The number of days detailed conversation history is stored. For this demo, you can keep the default 90 days.\n2. Types of Memory in AgentCore 1. Summarization – Conversation Summaries Function: Summarizes the conversation after it ends or periodically. Purpose: Keeps long-term context while using minimal storage.\nExample: You have 100 messages about AWS CLI errors → later the Agent remembers:\n“The user was experiencing AWS CLI connection issues.”\n2. Semantic Memory Function: Stores key facts or knowledge independent of context. Purpose: Used to answer questions based on previously mentioned information.\nExample:\n“Project A uses Python 3.9.” Ask again later → Agent responds with Python 3.9 immediately.\n3. User Preferences Function: Learns user habits and communication style. Purpose: Personalizes responses.\nExample: If you often say:\n“Keep the answer short.” The Agent will consistently respond concisely.\n4. Episodes Function: Stores sequences of events and analyzes success/failure through Reflections. Purpose: Helps the Agent learn from past experiences.\nExample: A previous flight booking failed due to missing dates → the Agent remembers. Next time, it asks for the date first.\n3. Memory Type Used in the Demo For the demo, you only need:\nSummarization Choose Summarization and click Create to complete the setup.\n4. Update Memory ID in Python After creating a Memory, you will receive a Memory ID.\nAdd it to your Python file:\n# AgentCore Memory Configuration REGION = \u0026#34;ap-southeast-1\u0026#34; MEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) Be sure to update the Memory ID and Region to match your configuration.\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.4-agent-core-run/5.4.1-run-agentcore/","title":"Configure &amp; Deploy AgentCore","tags":[],"description":"","content":"Getting Started with AgentCore Configure First, push your local code to AWS AgentCore using the command:\nagentcore configure -e ./{your_python_file.py} 1. Agent Name Enter a name for your Agent.\n2. Configuration File Press Enter to use the default configuration file pyproject.toml.\n3. Deployment Configuration Select 2 – Deploy using Docker, allowing AgentCore to automatically build and manage your Docker image.\n4. Execution Role Keep the default setting and let AWS create the IAM Role automatically.\n5. ECR Repository Press Enter to let AWS create the ECR repository for storing the Docker image.\n6. Authorization Configuration Choose No for OAuth. The Agent will only allow access via AWS IAM Access Key \u0026amp; Secret Key.\n7. Request Header Allowlist Press Enter to use the default allowlist configuration.\nResult Once this step is completed, your code has been successfully uploaded to AgentCore.\nLaunch the Agent Use the command below to start the Agent with your API Key (using GROQ):\nagentcore launch --env GROQ_API_KEY=your_api_key_here When the terminal shows Running, your Agent is successfully running.\n"},{"uri":"https://khanh-0.github.io/aws/3-blogstranslated/3.1-blog1/","title":"Dynamic Kubernetes Request Right Sizing with Kubecost","tags":[],"description":"","content":" Dynamic Kubernetes Request Right Sizing with Kubecost This article explores how to use the Kubecost Amazon Elastic Kubernetes Service (Amazon EKS) add-on to reduce infrastructure costs and optimize Kubernetes performance. The Container Request Right Sizing feature helps assess how container requests are configured, identify inefficiencies, and optimize them manually or automatically.\nWe will also cover how to evaluate right sizing recommendations and apply updates either once or on a schedule, helping your Amazon EKS environment stay continuously optimized.\nWhat is a Container Request? In Kubernetes, a container request is the minimum CPU and memory a workload declares so the scheduler can place the pod on an appropriate node.\nThe scheduler finds nodes with sufficient unused resources. Once a pod is placed, those resources are “reserved” even if the container doesn’t fully use them. Overly high requests → wasted resources and higher costs. Requests also affect the Quality-of-Service (QoS) tier and eviction decisions. Kubecost Savings Insights The Kubecost Amazon EKS add-on provides detailed visibility into containers that request excessive resources.\nThe Container Request Sizing dashboard shows:\nContainers that can be optimized Current CPU/memory request efficiency Average and peak CPU/memory usage Estimated monthly cost savings Total potential cluster savings Figure 1: Container Request Right Sizing recommendations\nRight sizing is especially effective in dev, test, and staging environments where performance requests are often “over-provisioned”.\nCustomizing Resize Recommendations Kubecost allows adjusting recommendations based on:\nWorkload type Criticality Operational goals You can choose prebuilt profiles such as:\ndevelopment production high availability Or create your own custom profile.\nAdjustable parameters include:\nCPU/memory target utilization Query window (48h, 7 days, …) Filter workloads by label, namespace, controller Acting on Kubecost Recommendations 1. One-time Resize Resize Requests Now in the dashboard applies the suggested requests immediately to:\nDeployments StatefulSets Jobs Reflecting the profile you selected. Figure 2: Enable Resize Requests and Enable Autoscaling\n2. Scheduled Right Sizing Select Enable Autoscaling to:\nSet up recurring resize jobs Update requests based on recent usage data Maintain optimized requests over time Example: A job runs every 2 hours, based on 48h of data, targeting 80% CPU/memory utilization.\n3. Automation via Helm You can enable automatic right sizing during Kubecost installation with Helm.\nExample configuration:\nclusterController: enabled: true actionConfigs: containerRightsize: filterConfig: - filter: | controllerKind:\u0026#34;deployment\u0026#34; schedule: start: \u0026#34;2024-08-20T00:00:00Z\u0026#34; frequencyMinutes: 120 recommendationQueryWindow: \u0026#34;48h\u0026#34; targetUtilizationCPU: 0.8 targetUtilizationMemory: 0.8 This configuration:\nRuns every 2 hours Targets 80% utilization Applies to all Deployments Ideal for platform teams wanting to enforce best practices automatically Conclusion Practical benefits from Kubecost request sizing:\nReduce 20–60% of compute costs in non-production environments Increase node utilization Faster pod scheduling Optimize performance and reduce bottlenecks Kubecost helps you:\nIdentify inefficient workloads Get data-driven recommendations Customize optimization strategies Apply changes manually or automatically If you haven’t used Kubecost yet, start at the Get Started page to install it in your EKS cluster.\nAbout the Authors | | Kai Wombacher Product Director at IBM Kubecost, specializing in optimizing large-scale Kubernetes costs. |\n| | Jason Janiak Partner Solutions Architect at AWS. |\n| | Mike Stefaniak Senior Director, Amazon EKS Product Team at AWS. |\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction to the Workshop This workshop provides a step-by-step guide to setting up IAM, AWS CLI, UV, Groq API, deploying RAG source code integrated with Groq LLM into AWS AgentCore, and finally publishing the API through AWS Gateway. Workshop Objectives \u0026ldquo;How to call APIs\u0026rdquo; — understand how to call external APIs outside AWS AgentCore. \u0026ldquo;Chunking\u0026rdquo; — learn how to split data for RAG so it can retrieve information optimally. \u0026ldquo;Adding memory to RAG\u0026rdquo; — explore how the RAG Agent can remember each piece of data during interactions with users. \u0026ldquo;Deploy AWS AgentCore\u0026rdquo; — understand how to deploy AWS AgentCore. \u0026ldquo;Publish API\u0026rdquo; — learn how to call AgentCore through an API. "},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.2-event2/","title":"Cloud Day AWS 2025 in HCMC","tags":[],"description":"","content":"Event Objectives Experience the plenary session live-streamed from Hanoi Featuring keynote speakers and breakthrough announcements that will shape Vietnam\u0026rsquo;s cloud future Generative AI: Explore the latest developments and practical applications Data Analytics: Transform your business through data-driven insights Migration \u0026amp; Modernization: Navigate your cloud transformation journey Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles – Vice President, Commercial Sales \u0026amp; Business Development APJ, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – Chief Executive Officer, Techcombank Trang Phung – CEO, U2U Network Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, Tymex Nguyen Van Hai – Director of Software Engineering, Techcombank Nguyen The Vinh – Co-Founder \u0026amp; CTO, Ninety Eight Nguyen Minh Ngan – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Key Highlights Vietnam Cloud Day 2025 – Hybrid Experience While the main event takes place in Hanoi, I was so excited to watch a seamless hybrid experience right here in Ho Chi Minh City.\nConnect with cloud innovators and industry leaders without leaving Ho Chi Minh City!\nExperience the plenary session live-streamed from Hanoi, featuring keynote speakers and breakthrough announcements that will shape Vietnam\u0026rsquo;s cloud future.\nFocus Areas \u0026amp; Interactive Sessions Ho Chi Minh City sessions will dive deep into three major themes:\nGenerative AI – Explore the latest developments and practical applications Data Analytics – Transform your business through data-driven insights Migration \u0026amp; Modernization – Navigate your cloud transformation journey Considerable Benefits Network locally: Meet and collaborate with Ho Chi Minh City\u0026rsquo;s vibrant tech community Learn from experts: Gain actionable knowledge from Vietnam’s top cloud leaders Stay ahead: Get firsthand updates on the future of cloud computing in Vietnam Key Takeaways Strategic Insights from Vietnam Cloud Day 2025 Cloud adoption momentum: Businesses across Vietnam are accelerating digital transformation Government collaboration: Opening remarks highlighted national cloud-first initiatives Customer success stories: Techcombank and U2U Network shared their journey to AWS adoption Executive leadership focus: Panel discussion emphasized aligning GenAI initiatives with business goals Technical Deep-Dive – Generative AI \u0026amp; Data Unified data foundation: Strategies for scalable, governed data pipelines on AWS GenAI adoption roadmap: Practical guidance on leveraging AWS services for AI-driven innovation AI-Driven Development Lifecycle (AI-DLC): Embedding AI throughout the software development process Security best practices: Protecting data, models, and apps with AWS’s layered security controls AI Agents: Moving beyond automation to intelligent, adaptive systems that multiply productivity Architecture \u0026amp; Operations Event-driven and modular design: Build resilient, loosely coupled systems Compute options: Choose between EC2, containers, and serverless based on workload requirements Scalability \u0026amp; observability: Design for growth with logging, monitoring, and cost controls Applying to Work Evaluate current workloads: Identify which applications can move to AWS for immediate ROI Build a data foundation: Start with ingestion, storage, and processing pipelines for analytics and AI Experiment with GenAI: Pilot use cases with Amazon Bedrock or SageMaker Strengthen security posture: Apply least-privilege IAM policies and secure network configurations Upskill teams: Encourage learning AWS AI/ML services to stay competitive Event Experience This is the first time I knew Eric Yeo - The AWS regional manager. He\u0026rsquo;s such a wonderful leader. Attending “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was an insightful and impactful experience, providing a clear roadmap for modernizing applications and databases using cutting-edge approaches and tools. My key takeaways include:\nLearning from Industry Leaders Gained valuable insights from AWS experts and executives from leading technology companies. Real-world case studies deepened my understanding of applying Domain-Driven Design (DDD) and Event-Driven Architecture to enterprise-scale projects. Hands-On Technical Experience Participated in event storming workshops, visualizing how to translate business processes into domain events. Practiced breaking down systems into microservices with clearly defined bounded contexts to reduce complexity. Explored trade-offs between synchronous vs. asynchronous communication, and learned when to apply pub/sub, point-to-point, and streaming patterns. Exploring Modern Tools Discovered Amazon Q Developer, an AI-powered assistant that supports the full Software Development Lifecycle (SDLC). Learned to automate code refactoring and implement serverless architectures using AWS Lambda to improve agility and delivery speed. Networking \u0026amp; Collaboration Connected with AWS specialists, business leaders, and fellow builders, strengthening the ubiquitous language between technical and business teams. Engaged in discussions that highlighted the importance of a business-first mindset for technology decisions. Key Lessons Learned Applying DDD and event-driven patterns significantly improves scalability, resilience, and maintainability. Successful modernization requires a phased, well-planned approach with clear ROI measurement to mitigate risks. Leveraging AI tools like Amazon Q Developer can dramatically accelerate development and streamline team workflows. Some event photos Overall, the event provided not just technical knowledge but also reshaped my perspective on application design, system modernization, and effective cross-team collaboration.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about AWS Virtual Private Cloud Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Go to office and meet new friends - Learn basics about Amazon VPC - Theory about subnets, Route tables, Internet Gateway, NAT gateway 09/15/2025 09/15/2025 3 - Theory about firewalls in VPC - Security groups, Network ACLs, VPC Resource map 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Do basic practice - Create VPC, Internet Gateway, Create route table, Create security group - Enable VPC flow Logs 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deploy Amazon EC2 + Create EC2 server + Create NAT Gateway + Use Reachability Analyzer - SSH connection methods to EC2 - Learn about Elastic IP 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn: + Configure Site to Site VPN + Clean up resources 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood AWS and mastered the basic service groups of Amazon VPC: Subnets Route tables Internet gateway NAT gateway Firewalls in Amazon VPC Deploying Amazon EC2 Instances "},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.4-agent-core-run/5.4.2-call-agentcore/","title":"Calling AgentCore","tags":[],"description":"","content":"Simple Demo with AgentCore 1. Send the first question Use the command:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Tell me about roaming activations\u0026#39;}\u0026#34; The Agent will respond based on the data you deployed (database + logic in your code).\n2. Test memory between invocations (session) After the first question, send another related one — for example:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Activate it for Vietnam\u0026#39;}\u0026#34; Then ask:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;which country was i referring to\u0026#39;}\u0026#34; If the Agent responds correctly and remembers the previous information → this confirms the Memory is working and AgentCore is maintaining context across invocations.\nAgentCore behaves as expected in this demo.\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.3-architecture/5.3.2-groq-api/","title":"Calling Groq API","tags":[],"description":"","content":" Objective Use the Groq library (ChatGroq / init_chat_model with model_provider=\u0026quot;groq\u0026quot;) to call an OpenAI-compatible model hosted on Groq.\nConfiguration in Code In the demo code:\nRetrieve API Key from Environment GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) The variable GROQ_API_KEY loads the API key from the environment variable.\nInitialize the Model llm = init_chat_model( model=\u0026#34;openai/gpt-oss-20b\u0026#34;, model_provider=\u0026#34;groq\u0026#34;, api_key=GROQ_API_KEY ) Integrate with Agent The Agent calls the LLM through create_agent(...) using the model=llm parameter:\nagent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Processing Flow Agent → Groq API → Model Inference → Response "},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.2-prerequiste/","title":"Preparation Steps","tags":[],"description":"","content":" IAM Permissions Required Permissions AdministratorAccess AmazonBedrockFullAccess AWSCodeBuildAdminAccess AWSCodeBuildDeveloperAccess BedrockAgentCoreFullAccess Create a User and Assign Permissions Go to IAM → Users → select Create user. Add the permissions listed above. Complete the user creation and save the Access Key if you need it for the SDK. Download AWS CLI Download AWS CLI: AWS CLI Link\nThen install it following the instructions.\nUV Management Setup 1. Why use UV? UV is fast, lightweight, and manages environments better than pip.\n2. Install UV on Windows Run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Add UV to PATH:\n$env:Path = \u0026#34;C:\\Users\\leamo\\.local\\bin;$env:Path\u0026#34; Restart your machine to apply the new PATH.\n3. Initialize a UV Environment Inside your project directory:\nuv init Then select the environment in VS Code.\nConnect Your Machine to AWS CLI Go back to IAM to create an Access Key.\nCreate Access Key and Configure AWS CLI In the user page: Security credentials → Create access key Choose Command Line Interface (CLI) Configure AWS CLI Run:\naws configure Fill in:\nAWS Access Key ID AWS Secret Access Key Default region name (example: ap-southeast-1) Default output format json Start AWS CLI AgentCore Run:\nuv run which agentcore After running, it will download all necessary libraries for AWS AgentCore.\nCreate Groq API Go to Groq and create an API key as shown. These external tools support RAG and are integrated through AWS AgentCore.\n"},{"uri":"https://khanh-0.github.io/aws/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APT Magic A Serverless AI Platform for Personalized Image Generation and Social Interaction 1. Executive Summary APT Magic is a serverless AI-powered web application designed to enable users to generate, personalize, and share artistic content such as AI-generated images. The platform integrates with AI foundation models via Amazon Bedrock and provides a seamless web experience using Next.js (SSR) hosted on AWS Amplify.\nThe MVP version focuses on real-time image generation and sharing, while the Future Design aims to scale with Bedrock agentCore/SageMaker Inference, SQS/SNS, Secret Manager \u0026amp; CloudTrail and AWS MLOps pipelines for advanced model orchestration and automation.\nAPT Magic is currently developed as a modern, cost-efficient, and secure AWS-native architecture for small to medium user bases, with planned expansion into enterprise-grade AI orchestration.\n2. Problem Statement What’s the Problem? Most AI image generation platforms are costly, rely on opaque third-party APIs, and offer limited personalization.\nDevelopers and creators often face high latency, lack of transparent model management, and limited control over user data security.\nThe Solution APT Magic leverages AWS serverless architecture to deliver:\nReal-time AI image generation through Amazon Bedrock Stability AI models. Secure user authentication and content management using Amazon Cognito and DynamoDB. Scalable API handling via AWS Lambda and API Gateway. Low-latency global delivery with CloudFront CDN and WAF protection. Future upgrades will include SQS/SNS decoupling, Bedrock AgentCore/SageMaker Inference pipelines, and cost-efficient CI/CD via CloudFormation. transforming APT Magic into a fully automated MLOps platform.\n3. Solution Architecture MVP Architecture The MVP is a fully serverless architecture, focusing on scalability, maintainability, and cost-effectiveness.\nCore AWS Services:\nRoute53 + CloudFront + WAF — Secure global access and caching. Amplify (Next.js SSR) — Hosts the frontend and server-side rendering layer. API Gateway + Lambda Functions — Manage backend logic (image processing, subscription, post APIs). Amazon Cognito — User authentication and access control. Amazon S3 + DynamoDB — Data persistence and image storage. Amazon Bedrock — Integrates foundation model (Stability AI) for image generation. CloudWatch — Logging, and monitoring. Security\nWAF + IAM policies for traffic filtering and role-based access control. Future Design (Enhanced Architecture) In the next phase, APT Magic will evolve into an AI orchestration platform, introducing new layers for automation, resilience, and model lifecycle management.\nNew Services to be Added:\nAmazon SQS — For reliable message queuing between async Lambda tasks.\nAmazon SNS — For real-time event notifications to users or administrators.\nAmazon ElastiCache (Redis) — For rate limiting and caching of frequent inference requests.\nAmazon Bedrock AgentCore — For hosting custom fine-tuned models and managing model endpoints.\nCI/CD\nCloudFormation for infrastructure deployment and automation. 4. Technical Implementation Implementation Phases Phase 1 – MVP Deployment (Completed / Current)\nImplement Amplify (Next.js SSR) + API Gateway + Lambda. Integrate Bedrock Stability AI API. Deploy CI/CD via Gitlab CI/CD. Enable user authentication (Cognito) and storage (S3 + DynamoDB). Log and Monitor via Cloudwatch Phase 2 – Future Design Expansion\nIntroduce SQS/SNS to decouple. Add ElastiCache for request throttling and caching. Integrate Bedrock Agent to enhance AI Pipelines Connect GitLab Runner with CodeBuild for unified CI/CD. 5. Timeline \u0026amp; Milestones Phase Description Estimated Duration Deployment Milestone Month 1: Setup \u0026amp; Core API Deploy infrastructure (IaC), Cognito, API Gateway, DynamoDB, and foundational Lambda functions. 4 Weeks Core Backend operational, Auth/User Management completed. Month 2: AI Integration Integrate Claude Haiku 3 LLM on Amazon Bedrock (Stability AI), Replicate API, complete Image Processing functions. 4 Weeks Successful end-to-end AI image processing demo. Month 3: Front-end \u0026amp; CI/CD Develop UI/UX (Amplify/Next.js), finalize CI/CD pipelines, and configure Monitoring/Security (CloudWatch/WAF). 4 Weeks Full platform ready for user testing. Month 4: Optimization \u0026amp; Go-Live Perform performance testing (Stress Test), cost optimization, and Production deployment. 4 Weeks Go-Live (Official product launch). 6. Cost Estimate (AWS Pricing Estimate) Total Cost Monthly: $9.80 Upfront: $0.00 12 Months: $117.60 Service Overview Service Region Monthly Cost Upfront 12-Month Cost Notes Amazon Route 53 Asia Pacific (Singapore) $0.50 $0.00 $6.00 1 Hosted Zone, 1 domain, 1 linked VPC Amazon CloudFront Asia Pacific (Singapore) $0.00 $0.00 $0.00 No specific configuration AWS WAF Asia Pacific (Singapore) $6.00 $0.00 $72.00 1 Web ACL; 1 rule per ACL AWS Amplify Asia Pacific (Singapore) $0.00 $0.00 $0.00 Build instance: Standard (8GB/4vCPU); request duration 500ms AWS CloudFormation Asia Pacific (Singapore) $0.00 $0.00 $0.00 No extensions; no operations Amazon API Gateway Asia Pacific (Singapore) $0.13 $0.00 $1.59 10k requests/month; WebSocket message 1KB; request size 30KB AWS Lambda Asia Pacific (Singapore) $1.67 $0.00 $20.04 1 million invokes; x86; 512MB ephemeral storage Amazon CloudWatch Asia Pacific (Singapore) $0.85 $0.00 $10.22 1 metric; 0.5GB logs in; 0.5GB logs to S3 S3 Standard Asia Pacific (Singapore) $0.23 $0.00 $2.76 10GB storage; 20k PUT; 40k GET DynamoDB On-Demand Asia Pacific (Singapore) $0.42 $0.00 $5.04 1GB storage; 1KB item; on-demand mode Total (Estimate) — $9.80 $0.00 $117.60 Based on AWS Pricing Calculator Metadata Currency: USD Locale: en_US Created On: 12/9/2025 Share URL: AWS Calculator Link Legal Disclaimer: AWS Pricing Calculator provides estimates only; actual costs may vary based on usage. AI Model Pricing Model Resolution / Token Usage Quality Price per Request (USD) Notes Titan Image Generator v2 \u0026lt; 512×512 Standard 0.008 Fixed price per 1 image Titan Image Generator v2 \u0026lt; 512×512 Premium 0.01 Fixed price per 1 image Titan Image Generator v2 \u0026gt; 1024×1024 Standard 0.01 Fixed price per 1 image Titan Image Generator v2 \u0026gt; 1024×1024 Premium 0.012 Fixed price per 1 image Stable Diffusion 3.5 Large Any N/A 0.08 Fixed price per 1 image Claude (text + image) 40 input tokens + 1 image N/A 0.00195 Price for 1 request including text and 1 image 1024×1024 Additional Options Mode Augmentation Price (USD) text→img no augment 0.08 text→img with augment 0.08195 img→img no augment 0.012 img→img with augment 0.094 7. Risk Assessment Risk Impact Probability Mitigation AI model inference latency Medium High Use ElastiCache + SQS/SNS for async handling Cost increase from model calls High Medium Bedrock usage control, SageMaker autoscaling CI/CD misconfigurations Medium Low CloudFormation rollback policies Security vulnerabilities High Medium WAF, GuardDuty, PrivateLink, IAM least privilege Third-party API dependency Medium Medium Bedrock fallback to S3-stored inference results 8. Expected Outcomes Technical Outcomes: Complete serverless AI image generation workflow with secure CI/CD. Modular orchestration enabling rapid MLOps integration. Improved latency and reliability via caching and async workflows. Long-Term Value: A foundation for AI as a Service (AIaaS) platform expansion. Ready-to-scale MLOps framework with automated retraining. Reusable cloud infrastructure for future AI products. "},{"uri":"https://khanh-0.github.io/aws/3-blogstranslated/3.2-blog2/","title":"Unlocking Next-Generation AI Performance with Dynamic Resource Allocation on Amazon EKS &amp; EC2 P6e-GB200","tags":[],"description":"","content":" Unlocking Next-Generation AI Performance with Dynamic Resource Allocation on Amazon EKS \u0026amp; EC2 P6e-GB200 By Vara Bonthu, Nick Baker, and Chris Splinter – September 2, 2025\nThe rapid evolution of agentic AI and large language models (LLMs), especially reasoning models, has created unprecedented demands for computational resources. Modern advanced AI models span hundreds of billions to trillions of parameters and require massive compute power, large memory, and high-speed interconnects to operate efficiently.\nOrganizations developing applications for natural language processing, scientific simulations, 3D content generation, and multimodal reasoning need infrastructure that can scale from today’s hundreds-of-billion-parameter models to future trillion-parameter boundaries while maintaining performance.\nIn this article, we explore how Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB200 UltraServers transform distributed AI workloads through seamless integration with Kubernetes.\nAWS introduces the EC2 P6e-GB200 UltraServers to meet the growing demand for large-scale AI model training and inference. They represent a significant architectural breakthrough for distributed AI workloads. Moreover, the launch of the EC2 P6e-GB200 UltraServer includes support for Amazon EKS (Elastic Kubernetes Service), providing a Kubernetes-native environment to deploy and scale from hundreds-of-billion-parameter models to trillions of parameters as AI landscapes continue to evolve.\nThe Power Behind the P6e-GB200: NVIDIA GB200 Grace Blackwell Architecture At the core of EC2 P6e-GB200 UltraServers is the NVIDIA GB200 Grace Blackwell Superchip, which integrates two NVIDIA Blackwell GPUs and one NVIDIA Grace CPU. Additionally, it provides NVLink Chip-to-Chip (C2C) connectivity between these components, delivering 900 GB/s bidirectional bandwidth—significantly faster than traditional PCIe interfaces.\nWhen deployed at rack scale, EC2 P6e-GB200 UltraServers participate in NVIDIA’s NVL72 architecture, creating memory-coherent domains up to 72 GPUs.\nFifth-generation NVLink enables GPU-to-GPU communication across hosts within the same domain at up to 1.8 TB/s per GPU. A key enabler of this performance is the Elastic Fabric Adapter (EFAv4) network, providing total network bandwidth up to 28.8 Tbps per UltraServer.\nEFA, combined with NVIDIA GPUDirect RDMA, enables GPU-to-GPU communication across hosts with low latency, bypassing the operating system. This ensures that the distributed GPU fabric operates with near-local memory performance across nodes.\nThis marks a significant improvement over previous EC2 P6-B200 UltraServers, which offered up to eight B200 Blackwell GPUs on x86 PCIe-based platforms. The P6e-GB200 upgrades the architecture by providing truly unified memory across racks—a critical requirement for training and operating trillion-parameter models efficiently.\nFigure 1: Amazon EC2 P6e-GB200 UltraServers\nUnderstanding EC2 P6e-GB200 UltraServer Architecture An EC2 P6e-GB200 UltraServer is not a single EC2 instance. Instead, it consists of multiple EC2 instances connected to operate as a unified entity:\nu-p6e-gb200x36: 36 GPUs distributed across 9 EC2 instances u-p6e-gb200x72: 72 GPUs distributed across 18 EC2 instances Each P6e-GB200 instance provides 4 NVIDIA Blackwell GPUs. Therefore:\nA u-p6e-gb200x36 UltraServer = 9 instances (9 × 4 = 36 GPUs) A u-p6e-gb200x72 UltraServer = 18 instances (18 × 4 = 72 GPUs) In Amazon EKS, each EC2 instance appears as a separate Kubernetes node, but EKS understands topology location and treats them as part of the same UltraServer through topology-aware routing.\nIntegrating P6e-GB200 UltraServers with Amazon EKS The Amazon EKS team collaborated closely with NVIDIA from the outset to establish integration requirements for P6e-GB200 with Kubernetes worker and control plane nodes. Based on these requirements, we developed AMI (Amazon Machine Images) for ARM64 Amazon Linux 2023 with NVIDIA flavor.\nWe also pre-packaged binaries for Internal Node Memory Exchange/Management Service (IMEX) and installed the required NVIDIA driver version.\nFurthermore, Amazon EKS quickly supported Dynamic Resource Allocation (DRA) for users starting from Kubernetes 1.33 on EKS (this feature is still in beta in vanilla Kubernetes).\nInstances have been validated with NVLink via IMEX as well as via EFA to achieve optimal data flow within and between UltraServers. Internally, we leverage NVIDIA Collective Communications Library (NCCL) to abstract transport-level decisions from the application layer.\nChallenge: Running Distributed AI Workloads on Kubernetes Deploying tightly coupled GPU workloads across multiple traditional nodes presents challenges in Kubernetes. Kubernetes’ traditional resource allocation assumes hardware tied to each node, making GPU resource management and memory-coherent interconnects across nodes difficult.\nThis is common for large-scale training workloads, such as LLMs or computer vision models requiring many GPUs to operate in parallel.\nA traditional approach when requesting GPUs in a pod looks like this:\nresources: limits: nvidia.com/gpu: 2 This static approach works for local GPUs but cannot represent complex interconnect topologies or GPU-to-GPU communication channels required by distributed training frameworks.\nSolution: Kubernetes DRA and IMEX To address these challenges, Kubernetes introduces DRA (Dynamic Resource Allocation), an extension framework that goes beyond traditional CPU and memory to handle complex hardware topologies.\nAmazon EKS enabled DRA in Kubernetes 1.33, providing sophisticated GPU topology management that static GPU allocation cannot achieve.\nHow DRA Resolves Traditional GPU Allocation Limits Unlike static resource models (e.g., nvidia.com/gpu: 2)—where you request a fixed number of GPUs without considering topology—DRA allows applications to declaratively describe resource requests via ComputeDomain and ResourceClaims.\nThis fundamental shift enables Kubernetes to make intelligent resource decisions based on actual topology, considering NVLink connectivity, memory bandwidth, and physical distance automatically.\nCritically, DRA abstracts away complex manual configurations, such as IMEX setup, NVLink partition management, and low-level hardware initialization, which would otherwise require deep GPU cluster expertise.\nThe NVIDIA DRA Driver is the key connector between Kubernetes DRA APIs and underlying hardware, including two specialized kubelet plugins:\ngpu-kubelet-plugin: for advanced GPU allocation features compute-domain-kubelet-plugin: automatically orchestrates IMEX primitives When creating a ComputeDomain requesting 36 GPUs across 9 EC2 instances (each with 4 Blackwell GPUs) or 72 GPUs across 18 instances for a full UltraServer, the system automatically:\nDeploys the IMEX daemon Establishes gRPC communication between nodes Creates a memory-coherent domain with cross-node mappings Exposes device files in containers Topology-aware Scheduling \u0026amp; Memory Coherence When a node joins an EKS cluster, the control plane receives topology information via the EC2 topology API and labels Kubernetes nodes:\nEach P6e-GB200 node is automatically labeled with capacity block type (eks.amazonaws.com/capacityType=CAPACITY_BLOCK and eks.amazonaws.com/nodegroup=...) and detailed network topology labels (topology.k8s.aws/network-node-layer-1 through network-node-layer-4) These labels indicate physical location within the UltraServer network fabric When GPU Feature Discovery (GFD) is enabled in NVIDIA GPU Operator, it applies clique labels (nvidia.com/gpu.clique) to identify GPUs within the same NVLink domain.\nThis topology enables scheduling-aware workloads across or within UltraServer node groups.\nIMEX is the core capability that allows GPUs across different nodes to directly access each other’s memory over NVLink. When an IMEX channel is provisioned via Kubernetes and DRA through a ComputeDomain, it appears in the container as a device file (e.g., /dev/nvidia-caps-imex-channels/channel0).\nThis allows CUDA applications to operate as if all GPUs were on the same board.\nThis capability is critical for distributed training frameworks like MPI and NCCL, now achieving near \u0026ldquo;bare-metal\u0026rdquo; performance across node boundaries without custom configuration or code changes.\nNVLink 5.0 provides the bandwidth foundation to operate these channels at 1.8 TB/s bidirectionally per GPU.\nThis enables memory-coherent compute domains across racks—foundational for real-time multi-node AI systems. In the NVL72 architecture, up to 72 GPUs can connect in a single memory-coherent NVLink domain.\nGPUs are organized into cliques based on physical NVSwitch connections, with all GPUs in the same node belonging to the same clique and sharing a Cluster UUID.\nWith GFD enabled, nvidia.com/gpu.clique labels each node with NVL domain ID and clique ID (e.g., cluster-abc.0), enabling scheduling-aware topology using node affinity rules.\nWhen scheduling training across 9 nodes of u-p6e-gb200x36 or 18 nodes of u-p6e-gb200x72, the kube-scheduler ensures all nodes belong to the same NVLink domain for maximum bandwidth.\nWhile NVLink provides ultra-high bandwidth within a physical domain, EFA ensures low-latency, high-throughput communication between different UltraServers. EFA’s RDMA capability combined with GPUDirect allows GPUs to communicate directly across nodes, creating a hybrid architecture where intra-UltraServer uses NVLink and inter-UltraServer uses EFA.\nThis makes the P6e-GB200 ideal for training massive models that scale from single-rack deployments to multi-rack supercomputers while maintaining optimal performance characteristics at all scales.\nDRA Workload Scheduling Workflow The diagram below illustrates how Kubernetes DRA integrates with NVIDIA GB200 IMEX to deploy distributed AI training workloads across nodes.\nWhen a pod requests 8 GPUs for distributed training with properly configured affinity rules, the system orchestrates deployment through a coordinated process:\nUsers specify targeted pods by node affinity (nvidia.com/gpu.clique) Kube-scheduler places pods according to these affinity constraints DRA components manage resource allocation and node coordination NVIDIA driver handles GPU allocation and IMEX orchestration IMEX service ensures memory consistency across nodes via gRPC The result is seamless deployment across two nodes (each with 4 GPUs) within the same NVLink domain, enabling high-bandwidth, low-latency communication essential for large-scale AI training workloads.\nUsing P6e-GB200 with Kubernetes DRA on Amazon EKS This section provides step-by-step guidance to set up an EKS cluster with EC2 P6e-GB200 UltraServers to leverage the above capabilities.\nPrerequisites Before starting, ensure you have the following tools and access. Refer to the EKS User Guide.\nInstalled AWS CLI eksctl (version supporting EKS 1.33) kubectl helm EC2 Capacity Blocks access for P6e-GB200 instances Step 1: Reserve UltraServer P6e-GB200 Capacity P6e-GB200 UltraServers are available only through Capacity Blocks for machine learning (ML). You must reserve the UltraServer (not individual instances) before creating an EKS cluster.\nIn the AWS console:\nGo to EC2 Console → Capacity Reservations → Capacity Blocks\nSelect the UltraServers tab (not Instances)\nChoose one of:\nu-p6e-gb200x36 (36 GPUs across 9 instances) u-p6e-gb200x72 (72 GPUs across 18 instances) Complete the reservation for the desired time period\nStep 2: Create EKS Cluster Configuration File Create a file named cluster-config.yaml with the following content:\n# cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: p6e-cluster region: us-east-1 version: \u0026#39;1.33\u0026#39; iam: withOIDC: true managedNodeGroups: - name: p6e-nodegroup amiFamily: AmazonLinux2023 instanceType: p6e-gb200.36xlarge desiredCapacity: 9 # All 9 instances from the UltraServer (36 GPUs total) minSize: 9 maxSize: 9 labels: nvidia.com/gpu.present: \u0026#34;true\u0026#34; taints: - key: nvidia.com/gpu value: \u0026#34;true\u0026#34; effect: NoSchedule availabilityZones: [\u0026#34;us-east-1-dfw-2a\u0026#34;] # Enable EFA (mandatory for P6e-GB200 UltraServers) efaEnabled: true capacityReservation: enabled: true capacityReservationTarget: capacityReservationId: \u0026#34;cr-1234567890abcdef\u0026#34; # Replace with your reservation ID Step 3: Deploy EKS Cluster eksctl create cluster -f cluster-config.yaml This command creates an EKS 1.33 cluster with 9 p6e-gb200.36xlarge instances from your reserved UltraServer, with EFA network enabled to optimize GPU-to-GPU communication.\nStep 4: Deploy NVIDIA GPU Operator The NVIDIA GPU Operator is essential for GB200 instances as it manages the full GPU lifecycle—including runtime configuration and advanced features such as MIG.\nWith complex NVLink topology spanning multiple nodes, GPU Operator dynamically manages GPU resources, configures MIG, and handles interconnect relationships that static plugins cannot.\n# Add the NVIDIA GPU Operator Helm repository helm repo add nvidia https://nvidia.github.io/gpu-operator helm repo update # Deploy the NVIDIA GPU Operator with custom values cat \u0026lt;\u0026lt;EOF \u0026gt; gpu-operator-values.yaml # gpu-operator-values.yaml driver: enabled: false mig: strategy: mixed migManager: enabled: true env: - name: WITH_REBOOT value: \u0026#34;true\u0026#34; config: create: true name: custom-mig-parted-configs default: \u0026#34;all-disabled\u0026#34; data: config.yaml: |- version: v1 mig-configs: all-disabled: - devices: all mig-enabled: false # P4DE profiles (A100 80GB) p4de-half-balanced: - devices: [0, 1, 2, 3] mig-enabled: true mig-devices: \u0026#34;1g.10gb\u0026#34;: 2 \u0026#34;2g.20gb\u0026#34;: 1 \u0026#34;3g.40gb\u0026#34;: 1 - devices: [4, 5, 6, 7] mig-enabled: false devicePlugin: enabled: true config: name: \u0026#34;\u0026#34; create: false default: \u0026#34;\u0026#34; toolkit: enabled: false nfd: enabled: true gfd: enabled: true dcgmExporter: enabled: true serviceMonitor: enabled: true interval: 15s honorLabels: false additionalLabels: release: kube-prometheus-stack daemonsets: tolerations: - key: \u0026#34;nvidia.com/gpu\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeSelector: accelerator: nvidia priorityClassName: system-node-critical EOF # Install GPU Operator using values file helm install gpu-operator nvidia/gpu-operator \\ --namespace gpu-operator \\ --create-namespace \\ --version v25.3.1 \\ --values gpu-operator-values.yaml Step 5: Install NVIDIA DRA Driver The NVIDIA DRA driver is essential for P6e-GB200 UltraServers, providing capabilities beyond traditional GPU plugins.\nWhile the standard NVIDIA Device Plugin exposes individual GPUs as countable resources (nvidia.com/gpu: 2), the DRA driver extends two important capabilities:\nComputeDomain Management: DRA Driver manages ComputeDomains—an abstraction for Multi-Node NVLink (MNNVL) deployments Advanced GPU Allocation: Beyond counting GPUs, it allows dynamic GPU allocation, MIG devices, and scheduling-aware topology The DRA driver includes two kubelet plugins:\ngpu-kubelet-plugin: for advanced GPU allocation functions compute-domain-kubelet-plugin: orchestrates ComputeDomain Create a values.yaml for **NVIDIA DRA Driver`:\n# values.yaml --- nvidiaDriverRoot: / gpuResourcesEnabledOverride: true # Required to deploy GPU and MIG deviceclasses resources: gpus: enabled: true computeDomains: enabled: true controller: nodeSelector: null affinity: null tolerations: [] kubeletPlugin: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: node Then install the NVIDIA DRA driver:\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia helm repo update helm install nvidia-dra-driver-gpu nvidia/nvidia-dra-driver-gpu \\ --version=\u0026#34;25.3.0-rc.4\u0026#34; \\ --namespace nvidia-dra-driver-gpu \\ --create-namespace \\ -f values.yaml After installation, the DRA driver creates DeviceClass resources that allow Kubernetes to understand and allocate ComputeDomains. This enables advanced topology management for distributed AI workloads on EC2 P6e-GB200 UltraServers.\nStep 6: Verify DRA Resources Check if the DRA resources are available:\nkubectl api-resources | grep resource.k8s.io/v1beta1 # Output: # deviceclasses resource.k8s.io/v1beta1 false DeviceClass # resourceclaims resource.k8s.io/v1beta1 true ResourceClaim # resourceclaimtemplates resource.k8s.io/v1beta1 true ResourceClaimTemplate # resourceslices resource.k8s.io/v1beta1 false ResourceSlice kubectl get deviceclasses # Output: # NAME CAPACITY ALLOCATABLE ALLOCATED # compute-domain-daemon.nvidia.com 36 36 0 # gpu.nvidia.com 0 0 0 # mig.nvidia.com 0 0 0 Validate IMEX Channels Once the GPU Operator and DRA driver are configured, you can create IMEX channels to enable direct GPU memory access across nodes. The example below shows how a ComputeDomain resource automatically provisions the required IMEX infrastructure:\nCreate imex-channel-injection.yaml:\n# filename: imex-channel-injection.yaml --- apiVersion: resource.nvidia.com/v1beta1 kind: ComputeDomain metadata: name: imex-channel-injection spec: numNodes: 1 channel: resourceClaimTemplate: name: imex-channel-0 --- apiVersion: v1 kind: Pod metadata: name: imex-channel-injection spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvidia.com/gpu.clique operator: Exists containers: - name: ctr image: ubuntu:22.04 command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;ls -la /dev/nvidia-caps-imex-channels; trap \u0026#39;exit 0\u0026#39; TERM; sleep 9999 \u0026amp; wait\u0026#34;] resources: claims: - name: imex-channel-0 resourceClaims: - name: imex-channel-0 resourceClaimTemplateName: imex-channel-0 This YAML creates a ComputeDomain resource and references it from a pod. The ComputeDomain controller automatically generates the ResourceClaimTemplate, which the pod uses to access the IMEX channel. Behind the scenes, this triggers deployment of the IMEX daemon on the selected node and dynamically establishes the IMEX domain instead of requiring a static setup.\nApply and verify:\nYou should see the pod imex-channel-injection-... in Running state.\nkubectl apply -f imex-channel-injection.yaml # Confirm the pod that runs to configure the compute domain kubectl get pods -n nvidia-dra-driver-gpu -l resource.nvidia.com/computeDomain # Output: # NAME READY STATUS RESTARTS AGE # imex-channel-injection-zrrlw-b6dqx 1/1 Running 5 (2m34s ago) 4m5s # Confirm the IMEX channel is created kubectl logs imex-channel-injection # Output: # total 0 # drwxr-xr-x. 2 root root 60 Apr 22 00:15 . # drwxr-xr-x. 6 root root 380 Apr 22 00:15 .. # crw-rw-rw-. 1 root root 241, 0 Apr 22 00:15 channel0 # Show logs of the pod configuring IMEX for the compute domain kubectl logs -n nvidia-dra-driver-gpu -l resource.nvidia.com/computeDomain --tail=-1 # Output: # /etc/nvidia-imex/nodes_config.cfg: # 192.168.56.245 # IMEX Log initializing at: 4/22/2025 00:14:21.228 # [INFO] IMEX version 570.133.20 is running with configuration options # [INFO] GPU event successfully subscribed # [INFO] Connection established to node 0 with IP 192.168.56.245 The logs show IMEX initialization, gRPC setup between nodes, and confirmation that the unified memory domain is active. GPUs across nodes in the UltraServer can now access memory directly over NVLink, providing unprecedented performance for distributed AI workloads.\nMulti-Node IMEX Communication in Practice To illustrate how the DRA driver coordinates GPU communication across nodes, the next section deploys a multi-node MPI benchmark using IMEX channels for high-bandwidth GPU-to-GPU memory transfers across EC2 P6e-GB200 UltraServer nodes.\nDeploy Multi-Node MPI Job Create nvbandwidth-test-job.yaml:\n# nvbandwidth-test-job.yaml --- apiVersion: resource.nvidia.com/v1beta1 kind: ComputeDomain metadata: name: nvbandwidth-test-compute-domain spec: numNodes: 2 # Request 2 nodes for cross-node testing channel: resourceClaimTemplate: name: nvbandwidth-test-compute-domain-channel --- apiVersion: kubeflow.org/v2beta1 kind: MPIJob metadata: name: nvbandwidth-test spec: slotsPerWorker: 4 # 4 GPUs per worker node launcherCreationPolicy: WaitForWorkersReady mpiReplicaSpecs: Worker: replicas: 2 # 2 worker nodes template: spec: containers: - image: ghcr.io/nvidia/k8s-samples:nvbandwidth-v0.7-8d103163 name: mpi-worker resources: limits: nvidia.com/gpu: 4 # Request 4 GPUs per worker claims: - name: compute-domain-channel # Link to IMEX channel resourceClaims: - name: compute-domain-channel resourceClaimTemplateName: nvbandwidth-test-compute-domain-channel Apply with:\nkubectl apply -f nvbandwidth-test-job.yaml ComputeDomain creation and node selection: The DRA driver immediately selects nodes:\nIdentifies 2 nodes with available GB200 GPUs Ensures nodes belong to the same NVLink domain Creates the ComputeDomain resource IMEX domain establishment: DRA automatically:\nDeploys IMEX daemons on both nodes Configures cross-node gRPC channels Sets up shared memory mappings between GPUs The experiment shows that DRA transforms multi-node GPU clusters into a unified resource, enabling LLM training across UltraServer nodes with native GPU memory access while maintaining optimal performance. All 72 GPUs in a u-p6e-gb200x72 UltraServer appear as a single memory domain, with Kubernetes orchestrating IMEX automatically so data teams can focus on modeling instead of infrastructure.\nConclusion Amazon EC2 P6e-GB200 UltraServers on Amazon EKS represent a major advancement for users looking to train and deploy trillion-parameter AI models at scale. The combination of NVIDIA Grace Blackwell GPUs with NVLink, support from Amazon EKS, DRA, and NVIDIA tooling has made exascale AI computing accessible through familiar container management patterns.\nThe integration of IMEX channels and NVLink enables a unified GPU memory cluster across nodes and racks, breaking the traditional limits of node-local GPU computing. This architectural improvement unlocks new possibilities for:\nTraining foundation models with trillions of parameters Running multimodal AI workloads with real-time performance requirements Deploying complex inference pipelines with sub-second latency To get started with DRA on Amazon EKS, refer to the Amazon EKS AI/ML documentation for comprehensive guidance and explore the AI on EKS project, which provides DRA examples you can experiment with and deploy in your environment.\nSECURITY NOTE: The configurations presented in this article are basic examples intended to illustrate core functionality. In production environments, you should implement additional security controls. Contact your AWS account team for guidance on using P6e-GB200 on Amazon EKS.\nAbout the Authors Vara Bonthu is a Senior SA specializing in open-source data and AI on EKS at AWS, driving open-source initiatives and supporting customers across organizations. He has deep expertise in open-source technologies, data analytics, AI/ML, and Kubernetes, with extensive experience in development, DevOps, and architecture.\nChris Splinter is a Senior Product Manager on the Amazon EKS team, focusing on helping customers run AI workloads with Kubernetes.\nNick Baker is a Software Development Engineer on the Node Runtime team at Amazon EKS. He focuses on enhancing support for accelerated workloads and improving data-plane stability on EKS.\n"},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.3-event3/","title":"AI-Driven Development Session with Amazon Q Developer &amp; Kiro","tags":[],"description":"","content":"Event Objectives Understand how generative AI is transforming the software development lifecycle Learn how AI tools such as Amazon Q Developer and Kiro accelerate development workflows Explore how AI integrates into architecture, coding, testing, deployment, and maintenance Experience live demonstrations of AI-assisted development in real engineering scenarios Enhance productivity by automating undifferentiated heavy lifting tasks, enabling developers to focus on creativity and innovation Speakers Toan Huynh – Instructor, AWS GenAI Builder Club My Nguyen – Instructor, AWS GenAI Builder Club Coordinators Diem My – Program Coordinator Dai Truong – Event Coordinator Dinh Nguyen – Operations Coordinator Key Highlights Toan Huynh is a guy with a fire of inspiration, he instructs will whole of his heart and intellectual knowledge.\nMy Huynh was so helpful and answered each question from audiences very deeply and knowledgably.\nTransforming Software Development with Generative AI Generative AI marks a new era in software engineering, reshaping how developers learn, plan, create, deploy, and manage applications.\nThis session highlighted how AI-driven development enables:\nAutomation of repetitive engineering tasks Rapid prototyping and faster delivery cycles Improved code quality and security through AI-assisted reviews A shift toward higher-value, design-focused work for developers AI in the Software Development Lifecycle (AI-DLC) The session introduced how AI tools integrate across the entire SDLC:\nArchitecture planning Code generation and refactoring Test case creation and validation Deployment pipelines Maintenance and monitoring Through Amazon Q Developer and Kiro, participants gained clarity on how AI elevates developer capabilities.\nAgenda 2:00 PM – 2:15 PM | Welcoming \u0026amp; Introduction 2:15 PM – 3:30 PM | AI-Driven Development Lifecycle Overview \u0026amp; Amazon Q Developer Demonstration Instructor: Toan Huynh 3:30 PM – 3:45 PM | Break 3:45 PM – 4:30 PM | Kiro Demonstration Instructor: My Nguyen Key Takeaways Strategic Insights on AI-Driven Development AI accelerates software delivery by automating repetitive tasks Development teams can redirect effort into architectural design and problem-solving AI tools reduce time spent on debugging, refactoring, and documentation Organizations adopting AI-enhanced development gain measurable productivity improvements Practical Learnings – Amazon Q Developer Generating high-quality code from natural language prompts Automatically fixing errors and optimizing functions Writing unit tests and documentation with AI Enhancing DevOps workflows through AI-assisted CI/CD automation Practical Learnings – Kiro Using Kiro’s AI capabilities for system design and development Real-time suggestions for architecture, code structure, and best practices Improved code readability, maintainability, and consistency across teams Applying to Work Adopt AI tools to streamline daily development tasks Use Amazon Q Developer to prototype ideas rapidly and improve code quality Integrate Kiro into architecture design and planning processes Encourage teams to experiment with AI to enhance efficiency and reduce repetitive workload Begin exploring an AI-augmented SDLC to modernize engineering practices Event Experience Attending this AI-Driven Development session was a refreshing and inspiring experience. It showcased how generative AI is reshaping the developer workflow and unlocking new levels of productivity.\nLearning from Experts Gained deep insights from Toan Huynh and My Nguyen on practical AI adoption Understood how AI fits not only in coding, but across architecture and DevOps pipelines Hands-On AI Demonstrations Observed live flows where Amazon Q Developer refactored code, generated documentation, and produced tests instantly Experienced how Kiro assists with system planning and engineering decisions Collaboration \u0026amp; Networking Connected with members of the AWS GenAI Builder Club, sharing ideas about AI development Discussed use cases and opportunities for integrating AI into daily engineering tasks Key Lessons Learned AI-driven development dramatically boosts speed, accuracy, and productivity Developers should evolve from code writers to system designers and solution thinkers Embracing AI early provides competitive advantages in modern engineering teams Speaker - Toan Huynh\nSpeaker - My Nguyen\nOverall, the session reinforced a powerful message: AI will not replace developers, but developers who use AI will outperform those who don’t.\n"},{"uri":"https://khanh-0.github.io/aws/3-blogstranslated/3.3-blog3/","title":"How Strangeworks Uses Amazon Braket to Explore Aircraft Loading Optimization","tags":[],"description":"","content":" Original article published on the AWS Quantum Technologies Blog\nQuantum computing promises breakthroughs for solving complex problems across industries, although it remains an open question how practically useful this technology will be. In this blog, the team from Strangeworks, an AWS partner, evaluates different implementations of the QAOA (Quantum Approximate Optimization Algorithm) on the aircraft loading optimization problem posed by Airbus as part of last year’s Quantum Mobility Challenge.\nOverview of the Problem The aircraft loading problem presented by Airbus is a type of bin packing optimization, common in many industries such as travel, manufacturing, and logistics. These problems are hard to solve because the number of possibilities grows exponentially as variables increase, creating a huge solution space even for relatively small instances.\nWhy Use Quantum Computing? Quantum computers may be well-suited to solving these optimization problems, potentially offering speedups compared to classical approaches. However, since current quantum hardware is not yet superior to the best classical methods, hybrid approaches that combine quantum and classical processing are under active consideration.\nCurrent gate-based quantum hardware has not yet outperformed top classical technologies, but the benchmarking approach in this article achieves accurate results for problems up to 80 qubits/variables.\nQAOA Algorithms Used QAOA is a hybrid quantum-classical algorithm, meaning it leverages both quantum and classical computation, and is considered well-suited for the NISQ (noisy intermediate-scale quantum) era. The Strangeworks team based their work on three QAOA variants:\nStandard QAOA: Available in many open-source libraries, including the Braket algorithm library\nRelax-and-Round QAOA (QRR): A variant developed by the Rigetti Computing team\nStrangeworksQAOA: A proprietary variant with improvements in classical processing, outperforming standard QAOA for this problem\nUsing Braket Hybrid Jobs Combining quantum and classical resources through hybrid algorithms like QAOA enables the use of existing quantum computers alongside classical workflows. The team used Amazon Braket Hybrid Jobs, which allow the full QAOA workflow to be submitted as a single job.\nBenefits of Braket Hybrid Jobs Reduced queue time: The algorithm only waits in the Braket queue once, rather than waiting for each individual quantum circuit submission Cached circuit compilation: Subsequent circuits with similar structure reuse previously compiled circuits, saving time and cost Integration with Strangeworks Platform Users can access Braket and these features via the Strangeworks Compute platform. The platform provides an online job management system and downloadable SDK, allowing access to StrangeworksQAOA, Braket Hybrid Jobs, quantum and quantum-inspired devices, and classical solvers.\nBenchmark Methodology To benchmark StrangeworksQAOA, the team evaluated the aircraft loading optimization problem, initially formulated as part of the Airbus quantum computing challenge. The problem aims to maximize the total load on the aircraft while ensuring that the number of containers (n) fits within the available spaces (N).\nTest Cases Table 1: Values for number of containers (n) and available spaces (N)\nContainers (n) Spaces (N) Qubits/Variables 5 3 26 6 4 38 6 5 46 7 5 52 7 6 61 8 6 68 8 7 78 Comparison Results Figure 1: Comparison of the StrangeworksQAOA algorithm (solid circles) running on Rigetti Ankaa-3 QPU with Standard QAOA (solid squares), both using Braket Hybrid Jobs. Also shown are Rigetti QRR (hatched squares) and Strangeworks QRR (hatched circles).\nResults are averaged over 8 runs. Error bars are small relative to the plot width, indicating stable performance for the considered problem instances.\nGradient Analysis Table 2: Gradient of the best-fit line for each algorithm\nAlgorithm Gradient Gurobi -1.25 StrangeworksQAOA 3.47 Standard QAOA n.a Strangeworks QRR QAOA -0.16 Rigetti QRR QAOA 1.06 Standard QAOA grows exponentially, so a linear fit is not reasonable for that case.\nGradient analysis shows that the Strangeworks variant of QRR is closest to the exact result (Gurobi), indicating it maintains the smallest error as system size increases.\nDetails of StrangeworksQAOA QAOA is an iterative hybrid algorithm cycling between quantum computation and classical optimization.\nFigure 2: QAOA workflow - (a) parameterized quantum circuit setup; (b) measurement into bitstrings; (c) repeated iterations to build probability distribution; (d) cost function evaluation and classical optimization to generate new parameterized circuits.\nWorkflow Steps Quantum circuit parameterization: Start with a set of classical parameters θ. The team used Real Amplitude Quantum Circuits instead of the standard QAOA ansatz due to circuit depth limitations. Run on QPU: Circuits run on the Rigetti Ankaa-3 QPU, measuring qubits (0 or 1) multiple times to build a probability distribution. Cost function evaluation: The probability distribution is fed into a classical cost function. Optimization: Classical parameters are updated using COBYLA from SciPy. Differences in StrangeworksQAOA In Standard QAOA, the solution is usually chosen from the bitstring with the highest probability. For large systems, the probability distribution flattens and each state is only measured once.\nIn StrangeworksQAOA, the team:\nTracks the lowest cost (C_q) over iterations Reports the basis state with minimal cost as the solution This is reasonable since the aircraft loading problem has a single classical solution Cost Function Formulas Cost for basis state (q):\nAverage total cost:\nWhere:\n(q_i \\in {0,1}) is the i-th bit of basis state q (J_{i,j}) are the QUBO/graph coupling coefficients (N_q) is the number of times the basis state was measured Conclusions and Outlook Analysis demonstrates the value of optimizing quantum algorithms for specific applications. Key findings:\nStrangeworksQAOA is more robust against hybrid quantum-classical algorithm limitations The combination of the Strangeworks platform, StrangeworksQAOA, Braket Hybrid Jobs, and Rigetti Ankaa-3 hardware provides a practical path to solving optimization problems Careful tuning achieves superior results without additional cost Reliability and reproducibility on the aircraft loading problem suggest a promising path toward achieving practical quantum advantage.\nGetting Started To explore these algorithms for your own optimization challenge:\nVisit the Strangeworks platform See the QAOA documentation Start with Amazon Braket References Romero, S., Osaba, E., Villar-Rodriguez, E., Oregi, I. \u0026amp; Ban, Y. Hybrid approach for solving real-world bin packing problem instances using quantum annealers. Sci Rep 13, 11777 (2023). Farhi, E., Goldstone, J. \u0026amp; Gutmann, S. A Quantum Approximate Optimization Algorithm. arXiv:1411.4028 (2014). Dupont, M. \u0026amp; Sundar, B. Extending relax-and-round combinatorial optimization solvers with quantum correlations. PhysRevA.109.012429 (2024). Airbus Quantum Computing Challenge Pilon, G., Gugole, N. \u0026amp; Massarenti, N. Aircraft Loading Optimization \u0026ndash; QUBO models under multiple constraints. arXiv:2102.09621 (2021). Guerreschi, G. G. Solving Quadratic Unconstrained Binary Optimization with divide-and-conquer and quantum algorithms. arXiv:2101.07813v1 (2021). Gurobi Optimization QAOA Circuit Ansatz Real Amplitude Quantum Circuit Ansatz Braket QAOA Example About the Authors Stuart Flannigan Senior Quantum Software Engineer at Strangeworks, with experience analyzing quantum simulation experiments. Since joining Strangeworks three years ago, he has worked on integrating theoretical ideas from quantum systems into classical workflows and AI, making the technology more accessible to industry.\nAndrew J. Ochoa Chief Scientist at Strangeworks, leading R\u0026amp;D initiatives. Holds a Ph.D. in Physics from Texas A\u0026amp;M University and an MBA from UT McCombs. He has multiple publications in quantum computing and has worked at Strangeworks since 2018.\nMichael Brett Principal Specialist for Quantum Computing in the High Performance Computing group at AWS. Leads global business development and marketing efforts for Amazon Braket. Formerly SVP for Applications at Rigetti Computing and CEO of QxBranch.\nCharunethran Panchalam Govindarajan Sr. Product Marketing Manager at AWS, focusing on High-Performance Computing and Quantum Technologies. Holds a Master’s in Electrical Engineering from Stanford University. Outside work, enjoys drawing and philosophical discussions.\nPublished on September 2, 2025 in Amazon Braket, Quantum Technologies\n"},{"uri":"https://khanh-0.github.io/aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Dynamic Kubernetes Request Right Sizing with Kubecost This article explores how to use the Kubecost Amazon Elastic Kubernetes Service (Amazon EKS) add-on to reduce infrastructure costs and optimize Kubernetes performance. The Container Request Right Sizing feature evaluates how container requests are configured, identifies inefficiencies, and allows you to optimize them either manually or automatically.\nWe also look at how to assess right-sizing recommendations and implement updates either once or on a scheduled basis, helping an Amazon EKS environment maintain continuous optimal performance.\nBlog 2 – Unlocking Next-Generation AI Performance with Dynamic Resource Allocation on Amazon EKS \u0026amp; EC2 P6e-GB200 The rapid evolution of \u0026ldquo;agentic\u0026rdquo; AI and large language models (LLMs), especially inference-focused models, has created unprecedented demand for computing resources. Advanced AI models today range from hundreds of billions to trillions of parameters and require massive computational power, large memory, and high-speed interconnects to operate efficiently.\nOrganizations building applications for natural language processing, scientific simulation, 3D content generation, and multimodal inference need infrastructure that can scale from today’s hundred-billion-parameter models to future trillion-parameter models while maintaining performance.\nBlog 3 – How Strangeworks Uses Amazon Braket to Explore Aircraft Loading Optimization Quantum computing promises breakthroughs for solving complex problems across industries, although its practical usefulness remains an open question. In this blog, the team from Strangeworks, an AWS partner, evaluates different implementations of the QAOA (Quantum Approximate Optimization Algorithm) on the aircraft loading problem posed by Airbus as part of last year’s Quantum Mobility Challenge.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master Amazon EC2 and its basic features. Understand EC2 instance types, AMI, and pricing models. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon EC2 Fundamentals + EC2 instance types (General Purpose, Compute Optimized, Memory Optimized) + AMI (Amazon Machine Images) + Key pairs and Security Groups 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - EC2 Hands-on: + Launch EC2 instances (Linux and Windows) + Connect via SSH/RDP + Configure Security Groups + Use User Data scripts 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn EC2 Advanced Features + Launch Templates + Create custom AMI + EC2 Instance Metadata + Placement Groups 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deploy applications on EC2 + Deploy web application on Amazon Linux + Deploy application on Windows Server 2022 + Configure basic Load Balancer 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn EC2 Pricing Models + On-Demand, Reserved, Spot Instances + Savings Plans + Cost optimization strategies + Clean up resources 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Mastered Amazon EC2:\nEC2 instance types and use cases AMI (Amazon Machine Images) and how to create custom AMI Key pairs and Security Groups EC2 User Data for bootstrapping instances EC2 Instance Metadata service Understood EC2 Advanced Features:\nLaunch Templates for standardizing deployments Placement Groups (Cluster, Spread, Partition) EC2 Instance Connect Elastic IP addresses Successfully practiced:\nLaunch and connect to EC2 instances (Linux and Windows) Configure Security Groups and Network ACLs Create custom AMI from running instance Deploy web applications on EC2 Use User Data scripts to automate setup Understood EC2 Pricing Models:\nOn-Demand Instances (pay as you go) Reserved Instances (1 or 3 years) Spot Instances (cheap but can be terminated) Savings Plans Cost optimization best practices "},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.3-architecture/5.3.3-chunking/","title":"Chunking &amp; Embedding","tags":[],"description":"","content":" Why Chunking Is Needed Documents or FAQs are often long; to compute embeddings effectively and optimize similarity search, large text must be split into smaller segments (chunks).\nBenefits Reduces context loss during embedding More accurate retrieval with vector similarity Better performance for search Compatible with token limits of embedding models Chunking Strategy The code uses RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Chunking Parameters Parameter Value Meaning chunk_size 500 Size of each chunk (characters) chunk_overlap 0 No overlap between chunks Optimization Tips Recommended chunk size: 500–1000 tokens (depends on embedding model)\nChunk overlap: Use 50–100 characters if continuous context is needed\nTrade-off:\nSmaller chunks → higher accuracy but more vectors Larger chunks → fewer vectors but may lose context Creating Embeddings and Vector Store Initialize Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Selected model: all-MiniLM-L6-v2\nLightweight and fast Good for English Embedding dimension: 384 Create FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) provides:\nHigh-speed vector search Efficient handling of large datasets Multiple index algorithms Query the Vector Store results = faq_store.similarity_search(query, k=3) Parameters:\nquery: User question k=3: Returns top 3 most relevant chunks Important Notes Updating Data If documents change (add/update):\nRe-embed all data, or Apply incremental updates to the vector store Choosing an Embedding Model Trade-off comparison:\nCriteria Lightweight Model Heavy Model Speed Fast Slow Accuracy Good Very good Cost Low High Use cases FAQ, chatbot Research, legal Vietnamese Models For better Vietnamese performance:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Full Code Example from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;How do I change my password?\u0026#34; results = faq_store.similarity_search(query, k=3) Deployment Checklist Prepare documents (CSV, JSON, text files) Select appropriate chunk_size (test 500, 750, 1000) Choose embedding model (English or multilingual) Build and save FAISS index Test retrieval with sample queries Monitor and tune the k value for similarity search "},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.3-architecture/","title":"RAG Architecture Deployed on AWS AgentCore","tags":[],"description":"","content":" Using the Gateway Endpoint In this section, we will explore how to integrate Groq to call OpenAI-compatible models and how to perform data chunking for RAG.\n"},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.4-event4/","title":"CMC Global TechTalk Series – Cloud &amp; Digital Transformation","tags":[],"description":"","content":"Event Objectives Explore leading technologies applied across CMC Global’s digital transformation ecosystem Learn practical Cloud Engineering and Architecture insights from experienced industry professionals Understand real-world implementation of cloud migration, modernization, and enterprise solutions Provide students and developers with direct exposure to modern cloud practices and engineering workflows Strengthen community connection between CMC Global experts and future cloud builders Speakers Lê Thanh Đức – Cloud Delivery Manager, CMC Global Dư Quốc Thành – Technical Leader, CMC Global Văn Hoàng Kha – Cloud Engineer, AWS Community Builder Key Highlights Leading Technology Insights from CMC Global The TechTalk Series introduced participants to some of the most impactful technologies used in CMC Global’s comprehensive digital transformation solutions, covering:\nCloud-native deployment strategies Enterprise-level system modernization Cloud migration best practices Infrastructure automation and operational excellence Real-world case studies delivered to large-scale customers Each speaker shared unique perspectives based on years of hands-on experience in cloud delivery, technical leadership, and AWS community building.\nExpertise and Guidance from Industry Leaders Anh Lê Thanh Đức provided insights into cloud delivery management, customer engagement, and scaling cloud teams. Anh Dư Quốc Thành shared deep technical knowledge on solution design and solving enterprise engineering challenges. Anh Văn Hoàng Kha, as an AWS Community Builder, inspired attendees with cloud best practices and the mindset required to thrive in cloud engineering. Key Takeaways Many questions were given at the end of the meeting, which makes everyone very excited. At the time, I asked Mr. Duc for \u0026ldquo;How did he start his carear as a DevOps Engineer\u0026rdquo; and now I can know more how to be a DevOps Engineer.\nStrategic Insights on Cloud \u0026amp; Modernization Cloud transformation succeeds through strong architecture, planning, and team collaboration Organizations rely heavily on automation to ensure reliability, scalability, and operational efficiency Cloud governance, cost optimization, and security must be integrated from day one Becoming a cloud engineer requires continuous learning, hands-on experimentation, and community involvement Practical Learnings from the TechTalk Understanding how large enterprises structure cloud migration projects How CMC Global applies cloud-native patterns to deliver end-to-end digital solutions The importance of Infrastructure as Code (IaC), monitoring, and DevOps pipelines Real examples of solving performance bottlenecks and reliability issues in production systems Applying to Work Start with cloud fundamentals while practicing through real projects and hands-on labs Apply cloud-native principles such as microservices, serverless, and automation Use IaC tools like Terraform or AWS CDK to manage scalable infrastructure Develop skills in observability, cost control, and secure-by-design architectures Engage with cloud communities such as AWS User Groups and AWS Community Builder programs Event Experience Attending this CMC Global TechTalk Series was an inspiring experience, offering a clearer understanding of how cloud technologies are applied in real enterprises and how engineers solve complex problems in digital transformation projects.\nLearning from Industry Professionals Gained valuable knowledge from cloud managers, technical leaders, and AWS community builders Appreciated the transparency in sharing real challenges and lessons from actual customer projects Understood the mindset required to grow as a cloud engineer in modern tech environments Hands-On Perspectives Learned practical examples of cloud deployment, automation, and system modernization Understood how teams at CMC Global collaborate to deliver high-quality cloud solutions Collaboration \u0026amp; Networking Engaged with speakers and fellow participants to discuss cloud career paths Learned about opportunities within CMC Global’s engineering and delivery teams Key Lessons Learned Cloud engineering is a continuous journey of learning and experimentation Modern enterprises rely on scalable, secure, and automated infrastructure Community involvement accelerates growth and creates opportunities Event Banner\nOverall, the session provided valuable insights into enterprise cloud engineering, modern system design, and the importance of continuous learning in the digital transformation era.\n"},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"I have been an active organizer or an event maker at my uni for recent time. But AWS in HCMC makes me so impressed due to its professional and meaningful activities. The events given below are just some of them that I have hands on with. Watch more here:\nEvent 1 Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026amp; Time: 8:30 - 12:00, September 6, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Cloud Day AWS 2025 in HCMC\nDate \u0026amp; Time: September 18, 2025\nLocation: 26th-36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering.\nDate \u0026amp; Time: October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Reinventing DevSecOps with AWS Generative AI.\nDate \u0026amp; Time: October 16, 2025\nLocation: Online Meeting\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: November 17, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 7 Event Name: CloundFront as Your Foundation And AWS WAF \u0026amp; Application Protection\nDate \u0026amp; Time: November 19, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 8 Event Name: Game Day - Secret Agent(ic) Unicorns\nDate \u0026amp; Time: 2pm, November 21, 2025\nLocation: 26th Floor, AWS Vietnam Office\nRole: Attendee\nEvent 9 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 8:30 - 12:00, November 29, 2025\nLocation: 26th Floor, AWS Vietnam Office\nRole: Attendee\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master AWS IAM for access management and security. Understand EC2 Instance Storage and storage types. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS IAM + Users, Groups, Roles + Policies and Permissions + IAM best practices 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice IAM: + Create users and groups + Assign policies + Create and use IAM roles 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn EC2 Instance Storage + Amazon EBS (Elastic Block Store) + EC2 Instance Store + Amazon EFS (Elastic File System) 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice Storage: + Create and attach EBS volumes + Create snapshots + Use EFS for shared storage 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about backup and disaster recovery + EBS snapshots + AMI creation + Cross-region backup 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Mastered AWS IAM concepts:\nUsers, Groups, Roles Policies and permission assignment MFA (Multi-Factor Authentication) IAM security best practices Understood EC2 Instance Storage:\nEBS volumes and types (gp3, io2, st1, sc1) EC2 Instance Store (ephemeral storage) EFS for shared file systems Comparison between storage types Successfully practiced:\nCreating and managing IAM users, groups, roles Creating, attaching and managing EBS volumes Creating snapshots and restoring Setting up EFS and mounting to EC2 Understood backup strategies and disaster recovery for EC2 and storage.\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.4-agent-core-run/","title":"Run Agent Core","tags":[],"description":"","content":"In this section, you will learn how to deploy and invoke AWS Agent Core directly from your local machine. Why you should use AWS CLI Using the AWS CLI allows you to easily access, configure, and manage Agent Core from your own environment. It provides flexibility, convenience, and full control when working locally.\n"},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.5-event5/","title":"AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS","tags":[],"description":"","content":"Event Objectives Introduce participants to the fundamentals and practical applications of AWS AI/ML services Provide hands-on understanding of Amazon SageMaker for end-to-end machine learning workflows Explore the capabilities of Amazon Bedrock for building and deploying Generative AI applications Strengthen participants’ knowledge of prompt engineering, RAG architecture, and model selection Enable developers to understand industry-standard MLOps practices using AWS tools Create an opportunity for networking and collaboration among AI/ML enthusiasts Speakers AWS Vietnam AI/ML Specialist Team Guest Facilitators from AWS Training \u0026amp; Certification Key Highlights Understanding the AI/ML Landscape The workshop opened with an overview of the rapidly evolving AI and ML ecosystem in Vietnam, highlighting the increasing adoption of cloud-based AI platforms, enterprise demand for ML solutions, and the role of foundational models in modern applications.\nParticipants gained clarity on:\nThe accelerating growth of AI talent and industry needs Real-world use cases across finance, e-commerce, and digital transformation The importance of cloud platforms in scaling ML workloads Hands-On Deep Dive into AWS AI Services The workshop delivered a detailed walkthrough of the tools, frameworks, and best practices that AWS provides to accelerate ML development—from data preparation to deployment and monitoring.\nAgenda 8:30 AM – 9:00 AM | Welcome \u0026amp; Introduction\nParticipant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of Vietnam’s AI/ML landscape 9:00 AM – 10:30 AM | AWS AI/ML Services Overview\nAmazon SageMaker: End-to-end ML platform Data preparation and labeling workflows Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 AM – 10:45 AM | Coffee Break\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock\nFoundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt engineering: Chain-of-thought, few-shot prompting Retrieval-Augmented Generation (RAG) architecture \u0026amp; knowledge base design Bedrock Agents for multi-step workflows and tool integrations Guardrails configuration for safe and controlled AI output Live Demo: Building a Generative AI chatbot using Bedrock 12:00 PM | Lunch Break (Self-arranged)\nKey Takeaways Strategic Insights into AI/ML on AWS End-to-end ML pipelines become significantly faster with managed services like SageMaker Model deployment and monitoring require strong MLOps foundations to ensure reliability Choosing the right foundation model depends on accuracy, latency, and domain constraints Generative AI workloads demand strong governance and safety controls Practical Learnings from SageMaker Efficient data preparation through integrated labeling and processing tools Automated model optimization through hyperparameter tuning jobs Streamlined deployment with real-time endpoints and model monitoring The importance of versioning, lineage tracking, and reproducibility Practical Learnings from Bedrock How to apply prompt engineering patterns for optimal responses Understanding when to use Claude, Llama, or Titan based on workload Implementing Retrieval-Augmented Generation (RAG) to enhance model accuracy Using Bedrock Agents to orchestrate multi-step reasoning tasks Enforcing safety standards through Guardrails and output filtering Applying to Work Start experimenting with SageMaker Studio notebooks for ML model development Use Bedrock to rapidly prototype chatbots, assistants, and domain-specific AI tools Integrate RAG pipelines where accuracy and up-to-date information are critical Adopt MLOps best practices to improve reliability and scalability Continue developing AI/ML skills through workshops, labs, and AWS certifications Event Experience Attending this workshop at the AWS Vietnam Office was a highly inspiring experience. It provided a practical, hands-on view of how machine learning and generative AI systems are built and deployed at scale.\nLearning from AWS Experts Gained clear guidance on how SageMaker simplifies the ML lifecycle Learned how enterprises use Bedrock to accelerate GenAI adoption Understood how global best practices can be applied to real projects in Vietnam Hands-On Demonstrations Saw real examples of training and deploying ML models Experienced how Bedrock can build a functional chatbot in minutes Understood how RAG boosts model accuracy and practical usefulness Collaboration \u0026amp; Networking Connected with other developers, students, and cloud practitioners Exchanged insights on AI career paths and AWS learning opportunities Expanded my network within the AI/ML and cloud community in HCMC Key Lessons Learned ML and GenAI are most powerful when combined with strong engineering principles Productivity dramatically increases with managed AI services and automation Continuous experimentation is essential to mastering modern AI workloads Speaker - Presenter\nSpeaker - Hoang Anh\nSpeaker - Hieu Nghi\nOverall, the workshop provided deep insights into building real-world ML and GenAI applications, empowering participants with the knowledge and confidence to apply AWS AI services effectively in future projects.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand High Availability and Scalability in AWS. Master database services: RDS, Aurora, ElastiCache. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn High Availability \u0026amp; Scalability + Multi-AZ deployments + Auto Scaling Groups + Elastic Load Balancer (ALB, NLB, CLB) 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice HA \u0026amp; Scalability: + Create Auto Scaling Group + Configure Load Balancer + Test scaling policies 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Amazon RDS + RDS engines (MySQL, PostgreSQL, Oracle, SQL Server) + Multi-AZ and Read Replicas + Backup and restore 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Amazon Aurora \u0026amp; ElastiCache + Aurora MySQL/PostgreSQL + Aurora Serverless + ElastiCache (Redis, Memcached) 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Database: + Create RDS instance + Configure Multi-AZ + Create Read Replica + Use ElastiCache 10/03/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Mastered High Availability and Scalability concepts:\nMulti-AZ deployments Auto Scaling Groups and scaling policies Load Balancing (ALB, NLB, CLB) Health checks and monitoring Understood AWS database services:\nAmazon RDS and database engines Multi-AZ for high availability Read Replicas for read scalability Amazon Aurora and Aurora Serverless ElastiCache for caching (Redis, Memcached) Successfully practiced:\nSetting up Auto Scaling Group with Launch Template Configuring Application Load Balancer Creating and managing RDS instances Configuring Multi-AZ and Read Replicas Using ElastiCache to improve performance Understood database backup, restore and disaster recovery strategies.\n"},{"uri":"https://khanh-0.github.io/aws/5-workshop/5.5-clean/","title":"Clean up","tags":[],"description":"","content":"\u0026ndash;\nOverview In this section, we will remove everything that was created during this workshop.\nDeleting an Agent in AgentCore Runtime Go to Amazon Bedrock AgentCore in the AWS Console. In the left-hand menu, select Build → Runtime. In the Runtime resources list, you will see the agents you have created, for example: agent_demo_2 or agent_with_memory. Select the agent you want to delete by ticking the radio button in the Name column. Click the Delete button at the top right. Confirm the deletion when AWS prompts you. The agent will be permanently removed from the Runtime resources list. Deleting a Memory in AgentCore After deleting an agent in Runtime, you can manage or delete the Memory that the agent used:\nIn the left-hand menu, select Build → Memory. In the Memory resources list, select the memory you want to delete. Click the Delete button at the top right. Confirm the deletion when AWS prompts you. The memory will be removed from the system. "},{"uri":"https://khanh-0.github.io/aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a RAG Agent with Groq API and AgentCore Memory Overview In this workshop, we will build a complete RAG (Retrieval-Augmented Generation) Agent with the following capabilities:\nCalling the Groq API to use high-performance LLM models Chunking \u0026amp; Embedding documents for optimized vector search AgentCore Memory to maintain long-term context across chat sessions Tool Integration so the agent can automatically search FAQs and reformulate queries AgentCore provides a framework for building AI agents with memory persistence, middleware hooks, and tool orchestration — enabling the agent to “remember” conversation history and personalize responses. Contents Workshop Overview\nPrerequisites\nArchitecture\n5.3.1. Calling Groq API 5.3.2. Chunking \u0026amp; Embedding 5.3.3. AgentCore Code Handler Running AgentCore\nClean Up AgentCore\nTech Stack Component Technology LLM Provider Groq API (OpenAI models) Embedding Model HuggingFace (all-MiniLM-L6-v2) Vector Store FAISS Agent Framework LangChain + AgentCore Memory Backend AgentCore Memory Store Text Splitting RecursiveCharacterTextSplitter Prerequisites Python 3.8+ Groq API Key Basic understanding of RAG and LLMs Familiarity with vector embeddings Expected Outcome By the end of the workshop, you will have:\nAn agent capable of answering FAQs via vector search A memory system that remembers user preferences and context Tool orchestration allowing the agent to decide when to use tools Production-ready code with logging and error handling Get Started: 5.1. Workshop Overview\n"},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.6-event6/","title":"AWS Cloud Mastery Series #2: DevOps on AWS","tags":[],"description":"","content":"Event Objectives Introduce participants to modern DevOps principles and culture Provide practical knowledge of AWS DevOps tools across CI/CD, infrastructure, and operations Demonstrate how DevOps accelerates delivery through automation, observability, and continuous improvement Equip developers with hands-on insights into containerization, monitoring, and deployment strategies Strengthen understanding of reliability, scalability, and operational excellence on AWS Support participants in exploring DevOps career development and AWS certification pathways Speakers AWS Vietnam DevOps Specialist Team Guest Trainers from AWS Training \u0026amp; Certification Key Highlights Embracing the DevOps Mindset The workshop began with a discussion about the evolving shift toward a DevOps-centric culture. Participants were introduced to:\nThe principles of DevOps collaboration Key performance metrics (DORA metrics: Deployment Frequency, MTTR, Change Failure Rate) The importance of automation and continuous improvement How DevOps complements AI/ML workflows from previous sessions This session emphasized how a strong DevOps culture leads to faster delivery, higher reliability, and stronger engineering ownership.\nA Deep Dive into AWS DevOps Tooling Throughout the full-day workshop, participants explored AWS-native tools for CI/CD, Infrastructure as Code, container orchestration, and observability—gaining a solid understanding of how modern engineering teams build and deliver applications at scale.\nAgenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of the previous AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, GitFlow, Trunk-based development Build \u0026amp; Test: CodeBuild configuration, automated testing Deployment: CodeDeploy with Blue/Green, Canary, Rolling updates Orchestration: CodePipeline automation and workflow design Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, drift detection AWS CDK: Constructs, reusable patterns, multi-language support Demo: Deploying infrastructure using CloudFormation and CDK Discussion: How to choose between IaC tools 12:00 – 1:00 PM | Lunch Break (Self-arranged)\nAfternoon Session (1:00 PM – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, dashboards AWS X-Ray: Distributed tracing \u0026amp; performance insights Demo: Full-stack observability setup Best practices: Alerting, dashboards, on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment patterns: Feature flags, A/B testing Automated testing \u0026amp; CI/CD integration Incident management and postmortems Case studies from startups \u0026amp; large enterprises 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps career pathways AWS certification roadmap Key Takeaways Strategic Insights into DevOps on AWS DevOps accelerates delivery and enhances team collaboration through automation CI/CD pipelines reduce deployment risks and increase release frequency IaC ensures reliable, repeatable, and scalable infrastructure provisioning Containers and orchestration platforms standardize deployment environments Observability is essential for maintaining high availability and performance Practical Learnings from CI/CD AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline work together to form fully automated pipelines Deployment strategies like Canary and Blue/Green reduce downtime and risk Git strategies such as Trunk-based development improve delivery speed Practical Learnings from IaC CloudFormation provides controlled, declarative infrastructure management CDK enables flexible, code-driven infrastructure modeling Understanding how to combine both tools effectively is key for enterprise adoption Practical Learnings from Containers \u0026amp; Observability ECS and EKS offer powerful orchestration for microservice workloads App Runner simplifies deployment for containerized applications CloudWatch + X-Ray provides unified observability across the stack Applying to Work Build CI/CD pipelines to automate code builds, testing, and deployments Use IaC tools to maintain consistent infrastructure environments Adopt containers for scalable and portable application delivery Implement observability practices to improve reliability and reduce MTTR Continue learning DevOps tools and pursue AWS certifications Event Experience This DevOps on AWS workshop provided a comprehensive and practical view of modern DevOps workflows, giving participants a strong foundation for real-world engineering challenges.\nLearning from AWS DevOps Experts Gained valuable insights into AWS-native DevOps tooling Learned how enterprise DevOps transformations are executed Understood the real role of automation and observability in production environments Hands-On Demonstrations Observed a full CI/CD pipeline in action Saw how IaC simplifies provisioning and maintenance Compared multiple container deployment strategies Built monitoring dashboards and explored distributed tracing Collaboration \u0026amp; Networking Met engineers and developers passionate about DevOps Shared career insights, resources, and certification plans Expanded professional connections within the AWS DevOps community Key Lessons Learned DevOps is a combination of culture, tools, and operational discipline Automation is the path to speed, reliability, and efficiency Observability must be built in from the start—not added later Continuous learning is essential in fast-evolving DevOps environments Overall, this session strengthened my understanding of DevOps foundations, AWS tooling, and best practices—providing me with the confidence to design, automate, and operate modern cloud-native systems.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master Amazon Route 53 and DNS routing. Understand Classic Solutions Architecture and design patterns. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon Route 53 + DNS fundamentals + Hosted zones + Routing policies (Simple, Weighted, Latency, Failover, Geolocation) 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Route 53: + Register domain or use existing domain + Configure routing policies + Health checks 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Classic Solutions Architecture + 3-tier architecture + Stateless web tier + Stateful application tier 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Design sample architectures + WordPress on AWS + E-commerce platform + Microservices architecture 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Draw architecture with draw.io + Deploy a simple 3-tier architecture + Document architecture decisions 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Mastered Amazon Route 53:\nDNS fundamentals and how it works Hosted zones (public and private) Routing policies and use cases Health checks and DNS failover Domain registration Understood Classic Solutions Architecture:\n3-tier architecture (Web, App, Database) Stateless vs Stateful design Horizontal vs Vertical scaling Best practices for high availability Cost optimization strategies Successfully practiced:\nConfiguring Route 53 with routing policies Setting up health checks and failover Drawing AWS architecture with draw.io Deploying a simple 3-tier architecture Documenting and presenting architecture decisions Able to design and deploy basic architectures on AWS.\n"},{"uri":"https://khanh-0.github.io/aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired in school to a real working environment.\nI participated in the APT Magic project, through which I improved my skills in programming, planning, and AWS Cloud.\nRegarding work ethic, I always strived to complete assigned tasks well, follow internal rules, and actively communicate with colleagues to increase work efficiency.\nTo objectively reflect on my internship journey, I provide the following self-assessment based on key criteria:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge \u0026amp; Skills Industry understanding, applying knowledge to real work, tool usage skills, work quality ☐ ✅ ☐ 2 Learning Ability Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Following rules, work procedures, and being punctual ☐ ✅ ☐ 6 Growth Mindset Willingness to receive feedback and improve ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Collaborating effectively with colleagues and participating in group activities ☐ ✅ ☐ 9 Professional Conduct Respecting colleagues, partners, and the workplace environment ✅ ☐ ☐ 10 Problem-Solving Skills Identifying issues, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to Projects/Organization Work effectiveness, improvement initiatives, recognition from the team ✅ ☐ ☐ 12 Overall Evaluation General assessment of the entire internship process ✅ ☐ ☐ Areas for Improvement Be more open to presenting in front of larger groups instead of relying mainly on chat communication. Improve daily and professional communication skills, especially in handling situations. "},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.7-event7/","title":"CloundFront as Your Foundation And AWS WAF &amp; Application Protection","tags":[],"description":"","content":"Event Objectives Introduce participants to the foundational concepts of Amazon CloudFront as a global CDN Provide practical understanding of how CloudFront improves performance, reliability, and cost efficiency Explore AWS WAF, Shield, and Bot Control for Layer 7 application protection Demonstrate modern security architectures for defending web applications and APIs Reinforce learning through an interactive Fun Quiz session Strengthen participants’ knowledge in building secure, scalable, and cost-optimized cloud applications Speakers Mr. Hung Gia for the CloudFront Session Mr. Julian for the WAF session Key Highlights Accelerating Applications with Amazon CloudFront The session began with an introduction to the challenges organizations face with web performance, cost unpredictability, and traffic spikes. Participants learned how Amazon CloudFront addresses these issues through:\nGlobal Edge Network with worldwide Points of Presence Multi-layer caching and Origin Shield Advanced performance techniques (HTTP/3, persistent connections, multiplexing) Origin protection via VPC Origins and Origin Access Control Built-in cost optimization features Real-world examples highlighted how CloudFront reduces latency, origin load, and operational costs while improving user experience at scale.\nStrengthening Security with AWS WAF \u0026amp; Shield The second part of the workshop focused on application security in the modern threat landscape, introducing:\nCommon attack vectors: OWASP Top 10, DDoS, bots, CVEs AWS WAF components (WebACLs, rules, rule groups, COUNT mode, rate-based rules) AWS Managed Rules for rapid protection Bot Control features and client interrogation techniques AWS Shield and the multi-layer DDoS defense system Security architectures using CloudFront + WAF + Shield for robust L7 protection Participants gained a deeper understanding of how to build secure and resilient applications using AWS edge services.\nAgenda Morning Session (CloudFront) 8:30 – 9:00 AM | Welcome \u0026amp; Introduction\nOverview of performance challenges in modern web applications Understanding CDN roles in global delivery 9:00 – 10:30 AM | Amazon CloudFront Deep Dive\nCDN caching behavior and optimization Global Edge Network overview Performance enhancements (HTTP/3, compression, persistent TCP) Origin protection strategies Multi-origin failover and routing Demo: CloudFront distribution setup and behavior analysis 10:30 – 10:45 AM | Break\nMidday Session (WAF \u0026amp; Shield) 10:45 AM – 12:00 PM | AWS WAF \u0026amp; Application Protection\nWAF rule types and best practices Rate-based rules for HTTP flood mitigation AWS Managed Rules overview Bot Control and client interrogation Shield Advanced capabilities Demo: Building a WAF WebACL for CloudFront 12:00 – 1:00 PM | Lunch Break (Self-arranged)\nAfternoon Session (Security Architecture \u0026amp; Fun Quiz) 1:00 – 2:30 PM | Security Architecture at the Edge\nCloudFront + WAF + Shield integration Origin cloaking and private VPC Origins Applying security layers for APIs \u0026amp; web apps Observability tools for security monitoring 2:30 – 2:45 PM | Break\n2:45 – 3:30 PM | Fun Quiz – CloudFront \u0026amp; WAF Challenge\nReal-world scenario questions Knowledge checks on caching, rules, and protection layers Interactive team-based quiz session 3:30 – 4:00 PM | Wrap-up \u0026amp; Q\u0026amp;A\nCareer path for AWS Edge \u0026amp; Security Specialists Learning roadmap for CloudFront, WAF, and AWS Security Key Takeaways I witness a technical guy with the humbled age at almost 60 - Mr. Julian and our amazing sifu - Mr. Hung Gia.\nStrategic Insights into Edge Performance \u0026amp; Security CloudFront significantly improves global performance through caching and optimized networking Origins can be shielded from direct public traffic using VPC Origins and OAC WAF enables precise, rule-based protection against L7 attacks Bot Control enhances detection of evasive bots through telemetry and behavioral tokens Shield provides automated DDoS mitigation with global edge protection Practical Learnings from CloudFront Multi-layer caching reduces origin cost and improves latency Failover and routing strategies enhance application resilience HTTP/3 and advanced compression deliver measurable speed improvements Practical Learnings from WAF COUNT mode is essential before enforcing blocking rules Rate-based rules effectively mitigate traffic floods Managed Rules accelerate security deployment Client interrogation helps identify sophisticated bot behavior Applying to Work Use CloudFront to minimize latency and offload origin infrastructure Protect web applications and APIs with WAF + Managed Rules Implement Shield for enhanced DDoS protection Adopt defense-in-depth strategies for edge and application layers Continuously observe user behavior and adapt rulesets proactively Event Experience This CloudFront, WAF \u0026amp; Security Essentials workshop delivered both theoretical depth and hands-on practical insights. It provided a clearer understanding of how enterprises build secure, performant, and cost-efficient applications at global scale.\nLearning from AWS Edge \u0026amp; Security Experts Understood how CloudFront optimizes performance for millions of users Learned how WAF and Shield mitigate complex real-world attacks Observed multiple architectural patterns used by enterprise customers Hands-On Demonstrations Configured CloudFront distribution behaviors Created and tested WAF rules in real scenarios Explored bot detection and telemetry-based defenses Collaboration \u0026amp; Networking Engaged with peers interested in cloud security and performance Discussed real use cases and career paths in AWS security domains Key Lessons Learned Performance and security must be designed together at the edge Proper caching strategy reduces both latency and cost Defense-in-depth with CloudFront + WAF + Shield provides strong protection Continuous learning is essential in the evolving security landscape Overall, this workshop strengthened my understanding of edge networking, security best practices, and AWS application protection tools—giving me the confidence to design secure, scalable systems with CloudFront and AWS WAF.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master Amazon S3 and storage features. Understand S3 security and advanced features. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon S3 Basics + Buckets and Objects + Storage classes (Standard, IA, Glacier) + Versioning 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn S3 Advanced Features + Lifecycle policies + Cross-region replication + S3 Transfer Acceleration + S3 Select 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn S3 Security + Bucket policies + IAM policies for S3 + Encryption (SSE-S3, SSE-KMS, SSE-C) + Access Control Lists (ACLs) 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice S3: + Create buckets and upload objects + Configure versioning + Set up lifecycle policies 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice S3 Security: + Configure bucket policies + Enable encryption + Setup cross-region replication 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Mastered Amazon S3:\nBuckets, objects and S3 fundamentals Storage classes and cost optimization Versioning and object lifecycle S3 performance optimization Understood S3 Advanced Features:\nLifecycle policies to automatically transition storage classes Cross-region replication for disaster recovery S3 Transfer Acceleration S3 Select and Glacier Select Mastered S3 Security:\nBucket policies and IAM policies Encryption options (SSE-S3, SSE-KMS, SSE-C) Access Control Lists (ACLs) S3 Block Public Access Pre-signed URLs Successfully practiced:\nCreating and managing S3 buckets Configuring versioning and lifecycle policies Setting up encryption and bucket policies Setting up cross-region replication Hosting static website on S3 "},{"uri":"https://khanh-0.github.io/aws/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.8-event8/","title":"Game Day – Secret Agent(ic) Unicorns","tags":[],"description":"","content":"Event Objectives Introduce participants to GenAI-powered problem-solving through an interactive GameDay experience Provide hands-on exposure to Amazon Bedrock, AgentCore, Knowledge Bases, Guardrails, and MCP Help players understand how AI agents collaborate with AWS services in real-world workflows Encourage teamwork across mixed skill levels to strengthen learning and problem-solving strategies Demonstrate how serverless, database, and search services integrate with GenAI applications Create a fun and engaging environment where participants earn points through missions, puzzles, and challenges AWS Services Used Amazon Bedrock (Foundation Models, Knowledge Bases, Guardrails) Amazon Bedrock AgentCore (Runtime, Memory, Code Interpreter, Observability) Strands Agents Model Context Protocol (MCP) Amazon DynamoDB Amazon OpenSearch Serverless Amazon Q Developer for CLI Target Audience Secret Agent(ic) AI GameDay is designed for a broad range of roles—including data scientists, ML practitioners, architects, developers, and operations engineers.\nParticipants should:\nBe familiar with navigating the AWS Console Benefit from teamwork across different skill levels Not require programming expertise (helpful but optional) Be encouraged to form teams mixing Beginners, Intermediates, and Experts for maximum collaboration Difficulty Each team should ideally contain 3–5 members with varied skills:\n1–2 Experts 1–2 Intermediate participants 1–2 Beginners This ensures balanced collaboration and allows players to learn from each other.\nKey Highlights GenAI-Powered Adventure The event transforms GenAI learning into an interactive mission where teams operate as Secret Agents tasked with solving challenges using AI agents, databases, knowledge retrieval, and observability tools.\nInstead of traditional lectures, participants progress through:\nMissions Time-limited puzzles Evidence analysis Incremental point-based challenges This unique format enhances both technical learning and team dynamics.\nHands-On Experience with Bedrock \u0026amp; AgentCore Players interact with several cutting-edge GenAI services:\nUsing Foundation Models for natural-language reasoning Retrieving facts and evidence with Knowledge Bases Ensuring safety and correctness with Guardrails Running multi-step agents using AgentCore Runtime \u0026amp; Memory Observing agent behavior, debugging, and tracing Storing mission data in DynamoDB Using OpenSearch Serverless for advanced retrieval These concepts mirror real-world AI workloads but are presented in a fun, gamified environment.\nTeam Collaboration \u0026amp; Strategy Success requires:\nClear communication Division of roles Shared note-taking and knowledge tracking Fast troubleshooting Adaptability Teams earn points not only for completing missions but also for efficiency, creativity, and technical accuracy.\nAgenda Intro Presentation \u0026amp; AWS Account Setup – 30 minutes\nGameDay rules and scoring Logging in to AWS accounts Overview of Bedrock, AgentCore, and supporting services Game Playtime – 120 minutes\nMissions and challenges begin Breaks included Evidence puzzles, agent tasks, and time-based scoring Teams race to accumulate the highest number of points Closing – 30 minutes\nSurvey distribution Announcing the Top 3 Winning Teams Group photo session Recap of key learning outcomes Key Takeaways Although I did not get any award, but this is a memorable event that makes me and my team be stronger and achieve more knowledge supporting for the next car\nGenAI Learning Hands-on experience using Bedrock Foundation Models Understanding how AgentCore manages memory, reasoning, and operations Observability techniques for tracing multi-step AI agent workflows Technical Skills Using Knowledge Bases and OpenSearch Serverless for retrieval Applying Guardrails for controlled and safe outputs Leveraging DynamoDB for storing mission artifacts Using Amazon Q Developer CLI for rapid development Team \u0026amp; Strategy Insights Collaboration across diverse skill levels accelerates learning Effective communication and planning improve mission success Time pressure helps simulate real-world problem-solving environments Applying to Work Apply GenAI agents to automate workplace tasks and workflows Use retrieval strategies (Knowledge Bases / OpenSearch) in real projects Adopt safe AI development practices with Guardrails Experiment with Bedrock Agents \u0026amp; AgentCore for building multi-step assistants Use Q Developer CLI to accelerate prototyping and engineering tasks Event Experience Participating in the Secret Agent(ic) Unicorns GameDay was both exciting and intellectually stimulating.\nThis wasn’t a typical workshop—it was an AI investigation adventure that required quick thinking, creative problem solving, and teamwork.\nLearning Through Play The gamified format made complex GenAI concepts easier to understand and apply.\nBy solving scenario-based missions, we learned how to:\nChain multiple AI agent steps Debug reasoning paths using observability tools Store, retrieve, and validate information Use MCP integrations and Bedrock Agents to complete challenges Collaboration \u0026amp; Team Spirit Each team member brought a unique strength—some focused on debugging, others on logic, others on navigating AWS services.\nThis diversity made the experience more engaging and productive.\nMemorable Closing Session Seeing the leaderboard, celebrating the top teams, and taking group photos made the event fun and rewarding.\nIt was a perfect blend of learning, competition, and community building.\nOverall, the GameDay was an unforgettable experience, combining GenAI learning with interactive missions that strengthened both my technical skills and collaborative mindset.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand CloudFront, Global Accelerator and AWS Storage Extras. Master AWS Integration \u0026amp; Messaging services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn CloudFront \u0026amp; Global Accelerator + CloudFront distributions + Origins and behaviors + Global Accelerator use cases 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice CloudFront: + Create CloudFront distribution + Configure S3 origin + Custom domain with SSL/TLS 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS Storage Extras + AWS Storage Gateway + FSx for Windows File Server + FSx for Lustre + AWS Backup 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Integration \u0026amp; Messaging + Amazon SQS (Standard, FIFO) + Amazon SNS + Amazon Kinesis + AWS Step Functions 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Messaging: + Create SQS queues + Create SNS topics + Connect SQS with SNS + Create Step Functions workflow 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Mastered CloudFront and Global Accelerator:\nCloudFront distributions and caching strategies Origins (S3, EC2, ALB, custom origins) Cache behaviors and TTL Global Accelerator for low latency Comparison: CloudFront vs Global Accelerator Understood AWS Storage Extras:\nAWS Storage Gateway (File, Volume, Tape) FSx for Windows File Server FSx for Lustre for HPC workloads AWS Backup for centralized backup Mastered AWS Integration \u0026amp; Messaging:\nAmazon SQS (Standard vs FIFO queues) Amazon SNS (pub/sub messaging) Amazon Kinesis (Data Streams, Firehose, Analytics) AWS Step Functions (workflow orchestration) EventBridge for event-driven architecture Successfully practiced:\nCreating and configuring CloudFront distribution Setting up custom domain with SSL/TLS Creating SQS queues and SNS topics Connecting messaging services Creating Step Functions state machine "},{"uri":"https://khanh-0.github.io/aws/4-eventparticipated/4.9-event9/","title":"AWS Cloud Mastery Series #3: Security Pillar – AWS Well-Architected","tags":[],"description":"","content":"Event Objectives Introduce participants to the AWS Well-Architected Security Pillar Provide practical understanding of identity, detection, infrastructure protection, data protection, and incident response Strengthen awareness of real-world cloud security threats in Vietnam Explore modern IAM patterns, network segmentation, encryption strategies, and automated incident response Help participants understand how to apply Zero Trust principles and Defense-in-Depth in AWS environments Build foundational knowledge for pursuing AWS Security Specialty certification Target Audience This workshop is designed for:\nCloud engineers, security engineers, architects, DevOps engineers IT administrators and operations teams managing workloads on AWS Developers interested in security best practices Anyone who wants to strengthen their cloud security fundamentals Technical expertise is not strictly required, but familiarity with AWS Console is recommended.\nKey Highlights Security Foundation – Setting the Stage The session opened with an introduction to:\nThe role of the Security Pillar in the Well-Architected Framework Core security principles: Least Privilege, Zero Trust, Defense in Depth The Shared Responsibility Model and how responsibilities shift in managed services Real cloud security threats commonly observed in Vietnam This foundation helped participants understand why security must be built-in, not bolted on.\nDeep Dive into the 5 Security Pillars The workshop was structured around the five pillars of AWS Security, each with practical guidance, demos, and real examples.\nModern IAM architecture with SSO, SCPs, and permission boundaries Detection and continuous monitoring covering CloudTrail, GuardDuty, VPC Flow Logs, EventBridge Infrastructure protection using VPC segmentation, WAF, Shield, Network Firewall Data protection using encryption (KMS), secrets lifecycle management, and guardrails Incident response automation using Lambda and Step Functions Agenda Opening Session 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Role of Security Pillar in Well-Architected Core principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility Model Top cloud threats in Vietnam ⭐ Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture IAM users, roles, policies — avoiding long-term credentials IAM Identity Center: SSO and permission sets SCPs \u0026amp; permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM policies \u0026amp; simulate access ⭐ Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring\nCloudTrail (organization-level) GuardDuty \u0026amp; Security Hub Logging across all layers: VPC Flow Logs, ALB logs, S3 access logs Automation using EventBridge Detection-as-Code patterns 9:55 – 10:10 AM | Coffee Break\n⭐ Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security VPC segmentation: private vs public workloads Security Groups vs NACLs: when to use which WAF + Shield + Network Firewall Workload protection basics: EC2, ECS/EKS ⭐ Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation best practices Encryption at rest \u0026amp; in transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails ⭐ Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation AWS Incident Response lifecycle Playbooks for: Compromised IAM credentials S3 public data exposure EC2 malware detection Isolation, snapshots, evidence collection Auto-response with Lambda / Step Functions Wrap-Up Session 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 Security Pillars Common pitfalls in Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro) Key Takeaways Security Mindset Security must be continuous and proactive, not reactive Least privilege, Zero Trust, and Defense in Depth are foundational Understanding shared responsibility is essential for building secure systems IAM \u0026amp; Detection SSO + permission sets simplify multi-account access management SCPs enforce organization-level guardrails Continuous monitoring is critical with CloudTrail, GuardDuty, and Flow Logs Infrastructure \u0026amp; Data Protection Proper VPC segmentation reduces blast radius Encryption should be applied everywhere—at rest, in transit, across services Secrets lifecycle management is essential for preventing breaches Incident Response Automation dramatically speeds up containment and recovery Prepared playbooks are essential for real-world scenarios Evidence collection must be done securely and systematically Applying to Work Adopt IAM Identity Center and SCPs for secure multi-account operations Enable org-level CloudTrail and GuardDuty for unified monitoring Apply encryption and secrets rotation across all workloads Use WAF and Shield to protect applications from common L7 attacks Build automated IR flows using Lambda or Step Functions Event Experience Attending the AWS Well-Architected Security Pillar Workshop gave me a deeper understanding of how security should be designed, implemented, and automated across AWS environments.\nLearning from AWS Security Experts Gained practical insights into IAM hygiene, threat detection, and incident response Understood how modern organizations structure multi-account security Learned common pitfalls and how to avoid them in real customer environments Hands-On Guidance IAM policy validation exercises helped reinforce least-privilege concepts Seeing detection pipelines with EventBridge \u0026amp; GuardDuty made monitoring clearer Walkthroughs of IR playbooks demonstrated how automation accelerates recovery Community \u0026amp; Engagement Participants shared real-world security challenges Discussions highlighted industry trends and common misconfigurations The workshop reinforced the importance of continuous learning in cloud security Overall, this workshop strengthened my understanding of cloud security foundations, giving me the confidence to design secure, resilient systems aligned with the AWS Well-Architected Framework.\n"},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Master Containers and Serverless on AWS. Understand serverless architectures and best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Containers on AWS + Docker fundamentals + Amazon ECS (Elastic Container Service) + Amazon EKS (Elastic Kubernetes Service) 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Containers: + Create Docker image + Push image to ECR + Deploy container on ECS + Fargate vs EC2 launch types 10/28/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Serverless Overview + AWS Lambda fundamentals + API Gateway + DynamoDB + Lambda triggers and integrations 10/29/2025 10/29/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Serverless Architectures + Serverless web application + Event-driven architecture + Lambda best practices + SAM (Serverless Application Model) 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Serverless: + Create Lambda functions + Configure API Gateway + Connect with DynamoDB + Deploy serverless app 10/31/2025 10/31/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Mastered Containers on AWS:\nDocker fundamentals and containerization Amazon ECS (Fargate and EC2 launch types) Amazon EKS for Kubernetes workloads Amazon ECR (Elastic Container Registry) Task definitions and services Understood Serverless:\nAWS Lambda and event-driven computing Lambda triggers (S3, DynamoDB, API Gateway, etc.) Lambda layers and environment variables API Gateway (REST and HTTP APIs) DynamoDB for serverless databases Mastered Serverless Architectures:\nServerless web applications Event-driven architectures Lambda best practices (cold starts, memory, timeout) AWS SAM (Serverless Application Model) Serverless Framework Successfully practiced:\nBuilding and deploying Docker containers on ECS Creating and deploying Lambda functions Configuring API Gateway with Lambda Creating serverless CRUD application with DynamoDB Deploying serverless app with SAM/CloudFormation "},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master Databases, Data \u0026amp; Analytics services. Understand Machine Learning services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Databases in AWS + DynamoDB (NoSQL) + Amazon Redshift (Data Warehouse) + Neptune, DocumentDB, Timestream 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Databases: + Create DynamoDB table + Query and scan operations + DynamoDB Streams + Global Tables 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Data \u0026amp; Analytics + Amazon Athena + AWS Glue + Amazon EMR + Amazon Kinesis + QuickSight 11/05/2025 11/05/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice Data \u0026amp; Analytics: + Query S3 data with Athena + Create Glue crawler + Setup Kinesis stream 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Machine Learning + Amazon SageMaker + Rekognition, Comprehend + Translate, Polly, Transcribe + Amazon Lex 11/07/2025 11/07/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Mastered Databases in AWS:\nDynamoDB (NoSQL database) DynamoDB Streams and Global Tables Amazon Redshift (Data Warehouse) Amazon Neptune (Graph Database) Amazon DocumentDB (MongoDB compatible) Amazon Timestream (Time series database) Understood Data \u0026amp; Analytics:\nAmazon Athena (serverless query service) AWS Glue (ETL service) Amazon EMR (Elastic MapReduce) Amazon Kinesis (real-time data streaming) AWS Data Pipeline Amazon QuickSight (BI service) Mastered Machine Learning services:\nAmazon SageMaker (build, train, deploy ML models) Amazon Rekognition (image and video analysis) Amazon Comprehend (NLP service) Amazon Translate, Polly, Transcribe Amazon Lex (chatbots) Successfully practiced:\nCreating and querying DynamoDB tables Using Athena to query S3 data Setting up Glue crawler and ETL jobs Creating Kinesis stream for real-time data Using SageMaker and Rekognition "},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master AWS Monitoring, Security and Advanced Identity. Understand AWS VPC and networking. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Monitoring \u0026amp; Performance + Amazon CloudWatch (metrics, logs, alarms) + AWS CloudTrail + AWS Config + AWS Trusted Advisor 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Monitoring: + Create CloudWatch dashboards + Setup alarms and notifications + Enable CloudTrail + Review Trusted Advisor 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Advanced Identity \u0026amp; Security + AWS Organizations + AWS SSO + AWS KMS (Key Management Service) + CloudHSM, Secrets Manager 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Security Services + AWS Shield \u0026amp; WAF + Amazon GuardDuty + Amazon Inspector + AWS Security Hub 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Amazon VPC + Subnets, Route Tables, Internet Gateway + NAT Gateway, VPN + VPC Peering, Transit Gateway + VPC Endpoints 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Mastered AWS Monitoring \u0026amp; Performance:\nAmazon CloudWatch (metrics, logs, alarms, dashboards) AWS CloudTrail (audit and compliance) AWS Config (resource inventory and compliance) AWS Trusted Advisor (best practices recommendations) AWS X-Ray (distributed tracing) Understood Advanced Identity \u0026amp; Security:\nAWS Organizations (multi-account management) AWS SSO (Single Sign-On) AWS KMS (Key Management Service) AWS CloudHSM AWS Secrets Manager and Parameter Store AWS Certificate Manager Mastered AWS Security Services:\nAWS Shield (DDoS protection) AWS WAF (Web Application Firewall) Amazon GuardDuty (threat detection) Amazon Inspector (vulnerability assessment) AWS Security Hub (centralized security) Understood Amazon VPC:\nVPC components (subnets, route tables, IGW, NAT) Security Groups and NACLs VPC Peering and Transit Gateway VPN and Direct Connect VPC Endpoints (Gateway and Interface) Successfully practiced:\nSetting up CloudWatch monitoring and alarms Enabling CloudTrail and Config Configuring KMS encryption Creating and managing VPC with public/private subnets Setting up VPN connection "},{"uri":"https://khanh-0.github.io/aws/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master Disaster Recovery and Migration strategies. Review comprehensively and prepare for AWS Solutions Architect Associate. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Disaster Recovery \u0026amp; Migrations + DR strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site) + AWS Backup + AWS DRS (Disaster Recovery Service) 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn AWS Migration Services + AWS Application Discovery Service + AWS Migration Hub + AWS Database Migration Service (DMS) + AWS Server Migration Service (SMS) 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Review More Solutions Architecture + Hybrid cloud architectures + Multi-region architectures + Event-driven architectures + Well-Architected Framework 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 - Read AWS White Papers \u0026amp; Best Practices + AWS Well-Architected Framework + Security Best Practices + Cost Optimization + Reliability Pillar 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ 6 - Comprehensive review and practice tests: + Review all modules + Take practice exams + Complete final workshop 11/21/2025 11/21/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Mastered Disaster Recovery strategies:\nBackup \u0026amp; Restore (high RPO/RTO, low cost) Pilot Light (medium RPO/RTO) Warm Standby (low RPO/RTO) Multi-Site/Hot Site (very low RPO/RTO, high cost) AWS Backup and AWS Elastic Disaster Recovery Understood AWS Migration:\nAWS Application Discovery Service AWS Migration Hub AWS Database Migration Service (DMS) AWS Server Migration Service (SMS) AWS DataSync and Transfer Family 6 R\u0026rsquo;s of Migration (Rehost, Replatform, Repurchase, Refactor, Retire, Retain) Mastered More Solutions Architecture:\nHybrid cloud architectures Multi-region architectures Event-driven architectures Microservices patterns AWS Well-Architected Framework (5 pillars) Completed AWS learning path:\nStudied and practiced 29 modules Understood main AWS services Able to design AWS architectures Ready for AWS Solutions Architect Associate exam Completed final workshop and submitted via Drive/GitHub "},{"uri":"https://khanh-0.github.io/aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khanh-0.github.io/aws/tags/","title":"Tags","tags":[],"description":"","content":""}]